{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "f = F\n",
    "import torch.utils.data as Data\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import datetime\n",
    "import pickle\n",
    "import scipy.sparse as ss\n",
    "import logging\n",
    "LOG_FORMAT = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "logging.basicConfig(level=logging.INFO, format=LOG_FORMAT)\n",
    "import os\n",
    "\\\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '32'\n",
    "# import seaborn as sns\n",
    "\n",
    "import IPython.display as ipd\n",
    "import copy\n",
    "import random\n",
    "# from pandarallel import pandarallel\n",
    "# Initialization\n",
    "# pandarallel.initialize(progress_bar=True)\n",
    "# df.parallel_apply(func)\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "\n",
    "from transformers import *\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm \n",
    "from transformers.modeling_bert import BertConfig, BertEncoder, BertAttention,\\\n",
    "BertSelfAttention,BertLayer,BertPooler,BertLayerNorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dir = '../../var'\n",
    "my_var_dir = '../../var/hyr'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-17 15:42:36,071 - INFO - start read df data\n",
      "2020-07-17 15:43:55,878 - INFO - finish read df data\n"
     ]
    }
   ],
   "source": [
    "logging.info('start read df data')\n",
    "\n",
    "df_train_user = pd.read_csv('%s/data/train_semi_final/user.csv'% var_dir)\n",
    "pre_df_train_user = pd.read_csv('%s/data/train_preliminary/user.csv'% var_dir)\n",
    "df_train_user = pd.concat([pre_df_train_user, df_train_user])\n",
    "\n",
    "train_user = list(range(1, 3000001))\n",
    "test_user = list(range(3000001, 4000001))\n",
    "\n",
    "train_gender = np.array(list(df_train_user['gender']))\n",
    "train_age = np.array(list(df_train_user['age']))\n",
    "id_embedding_feq = np.load('%s/id_embedding_aa100_wv_glove.npy'%my_var_dir)\n",
    "df_user_info = pickle.load(open('%s/df_user_info.pickle'%my_var_dir, 'rb'))\n",
    "target_encode_user_dict, mp_target_encode = pickle.load(open('%s/target_info_new.pickle'%my_var_dir, 'rb'))\n",
    "\n",
    "logging.info('finish read df data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline = df_train_user.shape[0] < 30000\n",
    "if offline:\n",
    "    train_user = list(range(1, 301))\n",
    "    test_user = list(range(3000001, 3000101))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "ARG = namedtuple('ARG', [\n",
    "    'batch_size',\n",
    "    'epoch',\n",
    "    'lr',\n",
    "    'weight_decay',\n",
    "    'debug',\n",
    "    'n_embedding',\n",
    "    'max_length',\n",
    "    'n_eval',\n",
    "    'n_worker',\n",
    "    'device',\n",
    "    \n",
    "    'n_gpu',\n",
    "    'card_list',\n",
    "    \n",
    "    'n_fold',\n",
    "    'save_path',\n",
    "    'shuffle_flod',\n",
    "])\n",
    " \n",
    "args = ARG(\n",
    "    batch_size = 64 if offline else 256,\n",
    "    epoch = 10,\n",
    "    lr = 0.003,\n",
    "    weight_decay = 0.,\n",
    "    debug = False,\n",
    "    n_embedding = 100,\n",
    "    max_length = 128,\n",
    "    n_eval = 100000,\n",
    "    n_worker = 1,\n",
    "    device=torch.device(\"cuda:2\"),\n",
    "#     device=torch.device(\"cpu\"),\n",
    "\n",
    "    n_gpu = 2,\n",
    "    card_list = [0, 1],\n",
    "    \n",
    "    n_fold = 5,\n",
    "    save_path = '%s/model_one_aa100_wv_glove_lr0003_wd0_dp03/' % my_var_dir,\n",
    "    shuffle_flod = False,\n",
    "    \n",
    "    \n",
    ")\n",
    "\n",
    "if args.debug:\n",
    "    debug_number = 200000\n",
    "    sub_train_user = train_user[:debug_number]\n",
    "    sub_train_gender = train_gender[:debug_number] - 1\n",
    "    sub_train_age = train_age[:debug_number] - 1\n",
    "    sub_test_user = test_user[:debug_number]\n",
    "else:\n",
    "    sub_train_user = train_user\n",
    "    sub_train_gender = train_gender - 1\n",
    "    sub_train_age = train_age - 1\n",
    "    sub_test_user = test_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class AdDataset(Data.Dataset):\n",
    "    def __init__(self, user_ids, gender = None, age = None):\n",
    "        self.user_id = list(user_ids)\n",
    "        self.gender = gender if gender is not None else [-1 for _ in range(len(self.user_id))]\n",
    "        self.age = age if age is not None else [-1 for _ in range(len(self.user_id))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.user_id)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return [self.user_id[idx], self.gender[idx], self.age[idx]]\n",
    "    \n",
    "\n",
    "n_embedding = 100\n",
    "\n",
    "x_dict = {\n",
    "    'time' :  np.zeros((args.batch_size, args.max_length)).astype('int'),\n",
    "    'click_time' :  np.zeros((args.batch_size, args.max_length)).astype('int'),\n",
    "\n",
    "    'ad_id_wv' : np.zeros((args.batch_size, args.max_length, n_embedding)).astype('float32'),\n",
    "    'ad_id_glove' : np.zeros((args.batch_size, args.max_length, n_embedding)).astype('float32'),\n",
    "\n",
    "    'advertiser_id_wv' :  np.zeros((args.batch_size, args.max_length, n_embedding)).astype('float32') , \n",
    "    'advertiser_id_glove' : np.zeros((args.batch_size, args.max_length, n_embedding)).astype('float32'),\n",
    "\n",
    "    'x_len' :  np.zeros((args.batch_size,)).astype('long'),\n",
    "}\n",
    "\n",
    "\n",
    "x_target_encode = np.zeros((args.batch_size, args.max_length, 72)).astype('float32')\n",
    "def collate_fn(samples):\n",
    "    sample_np = np.array(samples)\n",
    "    user_ids = sample_np[:, 0]\n",
    "    gender = sample_np[:, 1]\n",
    "    age = sample_np[:, 2]\n",
    "    \n",
    "    \n",
    "    for i, user in enumerate(user_ids):\n",
    "        S = time.time()\n",
    "        df_user_info_sub = df_user_info.loc[user]\n",
    "        creative_ids = df_user_info_sub['creative_id'][-args.max_length:]\n",
    "        \n",
    "        len_data = len(creative_ids)\n",
    "        \n",
    "        x_dict['time'][i][:len_data] = df_user_info_sub['time'][:len_data]\n",
    "        x_dict['click_time'][i][:len_data] = df_user_info_sub['click_time'][:len_data]\n",
    "                \n",
    "        flod = target_encode_user_dict[user]\n",
    "        x_target_encode[i][:len_data] = np.array(list(mp_target_encode[flod].loc[creative_ids]))\n",
    "        \n",
    "        id_embeddings = id_embedding_feq[creative_ids]\n",
    "        x_dict['ad_id_wv'][i][:len_data] = id_embeddings[:, :100]\n",
    "        x_dict['ad_id_glove'][i][:len_data] = id_embeddings[:, 100:200]\n",
    "\n",
    "        x_dict['advertiser_id_wv'][i][:len_data] = id_embeddings[:, 200:300]\n",
    "        x_dict['advertiser_id_glove'][i][:len_data] = id_embeddings[:, 300:400]\n",
    "\n",
    "        x_dict['x_len'][i] = len_data\n",
    "        \n",
    "\n",
    "    len_user = sample_np.shape[0]    \n",
    "\n",
    "    if gender[0] == -1:\n",
    "        gender = None\n",
    "        age = None\n",
    "    else:\n",
    "        gender = torch.tensor(gender)\n",
    "        age = torch.tensor(age)\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'time' : torch.tensor(x_dict['time'][:len_user]).long(),\n",
    "        'click_time' : torch.tensor(x_dict['click_time'][:len_user]).long(),\n",
    "        \n",
    "        'ad_id_wv' : torch.tensor(x_dict['ad_id_wv'][:len_user]),\n",
    "        'ad_id_glove' : torch.tensor(x_dict['ad_id_glove'][:len_user]),\n",
    "\n",
    "        'advertiser_id_wv' : torch.tensor(x_dict['advertiser_id_wv'][:len_user]), \n",
    "        'advertiser_id_glove' : torch.tensor(x_dict['advertiser_id_glove'][:len_user]), \n",
    "\n",
    "        'target_encode_sequence' : torch.tensor(x_target_encode[:len_user]).float(),\n",
    "        'x_len' :  torch.tensor(x_dict['x_len'][:len_user]),\n",
    "        'gender' : gender,\n",
    "        'age' : age,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     31,
     43
    ]
   },
   "outputs": [],
   "source": [
    "TIME_FORWARD = 0\n",
    "TIME_BACKWARD = 0\n",
    "    \n",
    "\n",
    "def predict_batch_multi_task(model, user_ids, batch_size = args.batch_size):\n",
    "    len_user_ids = len(user_ids)\n",
    "    pre_list_gender = []\n",
    "    pre_list_age = []\n",
    "    pre_list_hidden = []\n",
    "    \n",
    "    train_dataset=AdDataset(user_ids)\n",
    "    data_loader = Data.DataLoader(\n",
    "        dataset=train_dataset,      \n",
    "        batch_size=args.batch_size,      \n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers = args.n_worker,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for step, data in enumerate(tqdm(data_loader)):\n",
    "            pre_gender, pre_age, pre_hidden = model(**data)\n",
    "            pre_list_gender.append(pre_gender.cpu().detach().numpy())\n",
    "            pre_list_age.append(pre_age.cpu().detach().numpy())      \n",
    "            \n",
    "    return {\n",
    "        'gender' : np.concatenate(pre_list_gender), \n",
    "        'age' : np.concatenate(pre_list_age),\n",
    "    }\n",
    "\n",
    "def eval_data(model, user_ids, gender_labels, age_labels):\n",
    "    choose_idx = list(range(len(user_ids)))\n",
    "    if(len(user_ids) > args.n_eval):\n",
    "        choose_idx = random.sample(choose_idx, args.n_eval)\n",
    "    ret_dict = predict_batch_multi_task(model, user_ids[choose_idx])\n",
    "\n",
    "    predict_gender = np.argmax(ret_dict['gender'], axis = 1)\n",
    "    predict_age = np.argmax(ret_dict['age'], axis = 1)\n",
    "    acc_gender = accuracy_score(gender_labels[choose_idx], predict_gender)\n",
    "    acc_age = accuracy_score(age_labels[choose_idx], predict_age)\n",
    "    return acc_gender, acc_age\n",
    "\n",
    "best_score = 0\n",
    "def train_multi_task(n_fold, model_class, class_parms, train_dataset, val_dataset, test_user_id):\n",
    "    \n",
    "    global TIME_FORWARD, TIME_BACKWARD, best_score\n",
    "    best_score = 0\n",
    "    \n",
    "    train_user_id = train_dataset['x']\n",
    "    train_gender = train_dataset['gender']\n",
    "    train_age = train_dataset['age']\n",
    "    \n",
    "    logging.info('train number %d, val number %d' % (len(train_user_id), len(val_dataset['x'])))\n",
    "    \n",
    "    torch_dataset = AdDataset(train_user_id, train_gender, train_age)\n",
    "    data_loader = Data.DataLoader(\n",
    "        dataset=torch_dataset,      \n",
    "        batch_size=args.batch_size,      \n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers = args.n_worker,\n",
    "    )\n",
    "    \n",
    "    model = model_class(**class_parms).to(args.device)\n",
    "            \n",
    "    \n",
    "    no_decay = [\"bias\", \"gamma\",\"beta\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr = args.lr, weight_decay = args.weight_decay)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=int(len(train_user_id)//(args.batch_size)), num_training_steps=int(len(train_user_id) / args.batch_size * args.epoch)\n",
    "    )\n",
    "\n",
    "    for epoch in range(args.epoch):\n",
    "        loss_list, loss_gender_list, loss_age_list = [], [], []\n",
    "        model.train()\n",
    "        \n",
    "        for step, data in enumerate(tqdm(data_loader)):\n",
    "            #forward\n",
    "            S = time.time()\n",
    "\n",
    "            loss, loss_gender, loss_age, pre_gender, pre_age, _ = model(**data)\n",
    "        \n",
    "            TIME_FORWARD += time.time() - S\n",
    "            \n",
    "            loss_list.append(float(loss))\n",
    "            loss_gender_list.append(float(loss_gender))\n",
    "            loss_age_list.append(float(loss_age))\n",
    "            \n",
    "            #backward\n",
    "            S = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 5)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            TIME_BACKWARD += time.time() - S\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        train_acc_gender, train_acc_age = eval_data(model, train_dataset['x'], train_dataset['gender'], train_dataset['age'])\n",
    "\n",
    "        val_acc_gender, val_acc_age = eval_data(model, val_dataset['x'], val_dataset['gender'], val_dataset['age'])\n",
    "        \n",
    "        if(val_acc_gender + val_acc_age > best_score):\n",
    "            torch.save(model, '%s/model_%d_best' % (args.save_path, n_fold))\n",
    "            best_score = val_acc_gender + val_acc_age\n",
    "\n",
    "        logging.info(\"flod %d epoch %d : \\n loss: %f loss_gender : %f, loss_age : %f, gender : %f, %f, age : %f, %f, score:%f\" %\\\n",
    "                     (n_fold, epoch, np.mean(loss_list), np.mean(loss_gender_list), np.mean(loss_age_list), \\\n",
    "                      train_acc_gender, val_acc_gender, train_acc_age, val_acc_age, val_acc_gender + val_acc_age))\n",
    "    \n",
    "    if(best_score - val_acc_gender - val_acc_age > 0.0):\n",
    "        model = torch.load('%s/model_%d_best' % (args.save_path, n_fold))\n",
    "\n",
    "    val_ret_dict = predict_batch_multi_task(model, val_dataset['x'])\n",
    "    test_ret_dict = predict_batch_multi_task(model, test_user_id)\n",
    "    \n",
    "    return model, val_ret_dict, test_ret_dict\n",
    "\n",
    "\n",
    "def nn_cross_validation_multi_task(x_train, gender, age, x_test, model_class, class_parms, func_train, is_cross = True, random_seed = 0):\n",
    "    \n",
    "    folds = KFold(n_splits=args.n_fold, shuffle=False)              \n",
    "    \n",
    "    if os.path.exists(args.save_path) == False:\n",
    "        os.mkdir(args.save_path)\n",
    "    \n",
    "    x_train_val = np.array(x_train)\n",
    "    gender_train_val = np.array(gender)\n",
    "    age_train_val = np.array(age)\n",
    "\n",
    "\n",
    "    score_gender_val = np.zeros((len(x_train), 2))\n",
    "    score_age_val = np.zeros((len(x_train), 10))\n",
    "    \n",
    "\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(folds.split(x_train, gender_train_val)):\n",
    "        save_trn_idx = trn_idx\n",
    "        save_val_idx = val_idx\n",
    "        \n",
    "        train_x, train_gender, train_age = x_train_val[trn_idx], gender_train_val[trn_idx], age_train_val[trn_idx]\n",
    "        val_x, val_gender, val_age = x_train_val[val_idx], gender_train_val[val_idx], age_train_val[val_idx]\n",
    "        train_dataset = {\n",
    "            'x' : train_x,\n",
    "            'gender' : train_gender,\n",
    "            'age' : train_age,\n",
    "        }\n",
    "        val_dataset = {\n",
    "            'x' : val_x,\n",
    "            'gender' : val_gender,\n",
    "            'age' : val_age\n",
    "        }\n",
    "        \n",
    "        model, val_ret_dict, test_ret_dict = func_train(n_fold, model_class, class_parms, train_dataset, val_dataset, x_test)\n",
    "        \n",
    "        score_gender_val[val_idx] = val_ret_dict['gender']\n",
    "        score_age_val[val_idx] = val_ret_dict['age']\n",
    "        \n",
    "        val_predict_gender = np.argmax(score_gender_val[val_idx] , axis = 1)\n",
    "        val_predict_age = np.argmax(score_age_val[val_idx] , axis = 1)\n",
    "\n",
    "        if is_cross == False:\n",
    "            eda_val_dict = {\n",
    "                'user' : val_x,\n",
    "                'pre_gender' : val_predict_gender,\n",
    "                'gender' : val_gender,\n",
    "                'pre_age' : val_predict_age,\n",
    "                'age' : val_age,\n",
    "                'score_gender' : score_gender_val[val_idx],\n",
    "                'score_age' :  score_age_val[val_idx]\n",
    "            }\n",
    "\n",
    "            return model, eda_val_dict, test_ret_dict\n",
    "        \n",
    "        acc_gender = accuracy_score(val_gender, val_predict_gender)\n",
    "        acc_age = accuracy_score(val_age, val_predict_age)\n",
    "\n",
    "        logging.info(\"%d : gender : %f, age: %f, score :%f\" % (n_fold, acc_gender, acc_age, acc_gender + acc_age))\n",
    "        \n",
    "        \n",
    "        pickle.dump(val_ret_dict, open('%s/val_dict_flod_%d.pickle' % (args.save_path, n_fold), 'wb'))\n",
    "        torch.save(model, '%s/model_%d' % (args.save_path, n_fold))\n",
    "        pickle.dump(test_ret_dict, open('%s/test_dict_flod_%d.pickle' % (args.save_path, n_fold), 'wb'))\n",
    "        \n",
    "        test_gender_pre = np.argmax(test_ret_dict['gender'], axis = 1) + 1\n",
    "        test_age_pre = np.argmax(test_ret_dict['age'], axis = 1) + 1\n",
    "        df_submit = pd.DataFrame()\n",
    "        df_submit['user_id'] = x_test\n",
    "        df_submit['predicted_gender'] = test_gender_pre\n",
    "        df_submit['predicted_age'] = test_age_pre\n",
    "        df_submit.to_csv('%s/df_submit_flod_%d.csv' % (args.save_path, n_fold), index=False)\n",
    "        \n",
    "        \n",
    "    return score_gender_val, score_age_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     11,
     15,
     33,
     70,
     78,
     82,
     96,
     199
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-17 15:53:42,947 - INFO - start training \n",
      "2020-07-17 15:53:43,311 - INFO - train number 2400000, val number 600000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb927a91bd144f59a78c823e72e56344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9375), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-169e6ec3ba3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    411\u001b[0m score_gender_val, score_age_val = nn_cross_validation_multi_task(sub_train_user, sub_train_gender, sub_train_age,\\\n\u001b[1;32m    412\u001b[0m                                                                     \u001b[0msub_test_user\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m                                                                     RE2, {'args' : re2_args}, train_multi_task, True)\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_gender_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s/score_gender_val.pickle'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-3c334a2b783d>\u001b[0m in \u001b[0;36mnn_cross_validation_multi_task\u001b[0;34m(x_train, gender, age, x_test, model_class, class_parms, func_train, is_cross, random_seed)\u001b[0m\n\u001b[1;32m    158\u001b[0m         }\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ret_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ret_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_parms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mscore_gender_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_ret_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gender'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-3c334a2b783d>\u001b[0m in \u001b[0;36mtrain_multi_task\u001b[0;34m(n_fold, model_class, class_parms, train_dataset, val_dataset, test_user_id)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mTIME_FORWARD\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mloss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mloss_gender_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_gender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mloss_age_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_age\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,features,eps=1e-6):\n",
    "        super(LayerNorm,self).__init__()\n",
    "        self.gamma=nn.Parameter(torch.ones(features))\n",
    "        self.beta=nn.Parameter(torch.zeros(features))\n",
    "        self.eps=eps\n",
    "    def forward(self,X):\n",
    "        mean=X.mean(-1,keepdim=True)\n",
    "        std=X.std(-1,keepdim=True)\n",
    "        return self.gamma*(X-mean)/(std+self.eps)+self.beta\n",
    "    \n",
    "class GeLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1. + torch.tanh(x * 0.7978845608 * (1. + 0.044715 * x * x)))\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, smoothing=0.1,weights=torch.ones(2)/2):\n",
    "\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        assert smoothing < 1.0\n",
    "        self.smoothing = smoothing\n",
    "        self.confidence = 1. - smoothing\n",
    "        self.weights=weights.to(args.device)\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        logprobs = F.log_softmax(x, dim=-1)\n",
    "#         new_target=torch.zeros(X.shape).scatter_(1,target.unsqueeze(1),1)\n",
    "#         smooth_target=new_target*0.9+torch.ones_like(new_target)*(0.1/new_target.shape[1])\n",
    "#         -(F.log_softmax(X,dim=-1)*smooth_target).sum(dim=-1).mean()\n",
    "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = (-logprobs*self.weights).sum(dim=-1)\n",
    "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
    "        return loss.mean()\n",
    "\n",
    "class Conv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_sizes):\n",
    "        super().__init__()\n",
    "        assert all(k % 2 == 1 for k in kernel_sizes), 'only support odd kernel sizes'\n",
    "        assert out_channels % len(kernel_sizes) == 0, 'out channels must be dividable by kernels'\n",
    "        out_channels = out_channels // len(kernel_sizes)\n",
    "        convs = []\n",
    "        for kernel_size in kernel_sizes:\n",
    "            conv = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                             padding=(kernel_size - 1) // 2)\n",
    "            nn.init.normal_(conv.weight, std=math.sqrt(2. / (in_channels * kernel_size)))\n",
    "            nn.init.zeros_(conv.bias)\n",
    "            convs.append(nn.Sequential(nn.utils.weight_norm(conv), GeLU()))\n",
    "        self.model = nn.ModuleList(convs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([encoder(x) for encoder in self.model], dim=-1)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, args, input_size):\n",
    "        super().__init__()\n",
    "        self.dropout = args.dropout\n",
    "        self.encoders = nn.ModuleList([Conv1d(\n",
    "                in_channels=input_size if i == 0 else args.hidden_size,\n",
    "                out_channels=args.hidden_size,\n",
    "                kernel_sizes=args.kernel_sizes) for i in range(args.enc_layers)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = x.transpose(1, 2)  # B x C x L\n",
    "        mask = mask.transpose(1, 2)\n",
    "        for i, encoder in enumerate(self.encoders):\n",
    "            x.masked_fill_(~mask, 0.)\n",
    "            if i > 0:\n",
    "                x = f.dropout(x, self.dropout, self.training)\n",
    "            x = encoder(x)\n",
    "        x = f.dropout(x, self.dropout, self.training)\n",
    "        return x.transpose(1, 2)  # B x L x C\n",
    "    \n",
    "class FullFusion(nn.Module):\n",
    "    def __init__(self, args, input_size):\n",
    "        super().__init__()\n",
    "        self.dropout = args.dropout\n",
    "        self.fusion1 = Linear(input_size * 2, args.hidden_size, activations=True)\n",
    "        self.fusion2 = Linear(input_size * 2, args.hidden_size, activations=True)\n",
    "        self.fusion3 = Linear(input_size * 2, args.hidden_size, activations=True)\n",
    "        self.fusion = Linear(args.hidden_size * 3, args.hidden_size, activations=True)\n",
    "\n",
    "    def forward(self, x, align):\n",
    "        x1 = self.fusion1(torch.cat([x, align], dim=-1))\n",
    "        x2 = self.fusion2(torch.cat([x, x - align], dim=-1))\n",
    "        x3 = self.fusion3(torch.cat([x, x * align], dim=-1))\n",
    "        x = torch.cat([x1, x2, x3], dim=-1)\n",
    "        x = f.dropout(x, self.dropout, self.training)\n",
    "        return self.fusion(x)\n",
    "    \n",
    "class AugmentedResidual(nn.Module):\n",
    "    def forward(self, x, res, i):\n",
    "        if i == 1:\n",
    "            return torch.cat([x, res], dim=-1)  # res is embedding\n",
    "        hidden_size = x.size(-1)\n",
    "        x = (res[:, :, :hidden_size] + x) * math.sqrt(0.5)\n",
    "        return torch.cat([x, res[:, :, hidden_size:]], dim=-1)  # latter half of res is embedding\n",
    "    \n",
    "class Pooling(nn.Module):\n",
    "    def forward(self, x, mask):\n",
    "        return x.masked_fill_(~mask, -float('inf')).max(dim=1)[0]\n",
    "\n",
    "class SumPooling(nn.Module):\n",
    "    def forward(self, x, mask):\n",
    "        return x.masked_fill_(~mask, 0).sum(dim=1)\n",
    "    \n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, activations=False):\n",
    "        super().__init__()\n",
    "        linear = nn.Linear(in_features, out_features)\n",
    "        nn.init.normal_(linear.weight, std=math.sqrt((2. if activations else 1.) / in_features))\n",
    "        nn.init.zeros_(linear.bias)\n",
    "        modules = [nn.utils.weight_norm(linear)]\n",
    "        if activations:\n",
    "            modules.append(GeLU())\n",
    "        self.model = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Alignment(nn.Module):\n",
    "    def __init__(self, args, __):\n",
    "        super().__init__()\n",
    "        self.temperature = nn.Parameter(torch.tensor(1 / math.sqrt(args.hidden_size)))\n",
    "        self.summary = {}\n",
    "\n",
    "    def _attention(self, a, b):\n",
    "        return torch.matmul(a, b.transpose(1, 2)) * self.temperature\n",
    "    \n",
    "    def add_summary(self, name, val):\n",
    "        if self.training:\n",
    "            self.summary[name] = val.clone().detach().cpu().numpy()\n",
    "\n",
    "    def forward(self, a, b, mask_a, mask_b):\n",
    "        attn = self._attention(a, b)\n",
    "        mask = torch.matmul(mask_a.float(), mask_b.transpose(1, 2).float()).byte()\n",
    "        attn.masked_fill_(~mask, -1e7)\n",
    "        attn_a = f.softmax(attn, dim=1)\n",
    "        attn_b = f.softmax(attn, dim=2)\n",
    "        feature_b = torch.matmul(attn_a.transpose(1, 2), a)\n",
    "        feature_a = torch.matmul(attn_b, b)\n",
    "        self.add_summary('temperature', self.temperature)\n",
    "        self.add_summary('attention_a', attn_a)\n",
    "        self.add_summary('attention_b', attn_b)\n",
    "        return feature_a, feature_b\n",
    "\n",
    "class MappedAlignment(Alignment):\n",
    "    def __init__(self, args, input_size):\n",
    "        super().__init__(args, input_size)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Dropout(args.dropout),\n",
    "            Linear(input_size, args.hidden_size, activations=True),\n",
    "        )\n",
    "\n",
    "    def _attention(self, a, b):\n",
    "        a = self.projection(a)\n",
    "        b = self.projection(b)\n",
    "        return super()._attention(a, b)\n",
    "\n",
    "class AlignmentOne(nn.Module):\n",
    "    def __init__(self, args, input_size):\n",
    "        super().__init__()\n",
    "        self.temperature = nn.Parameter(torch.tensor(1 / math.sqrt(args.hidden_size)))\n",
    "        self.summary = {}\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Dropout(args.dropout),\n",
    "            Linear(input_size, args.hidden_size, activations=True),\n",
    "        )\n",
    "        \n",
    "    def _attention(self, a):\n",
    "        a = self.projection(a)\n",
    "        return torch.matmul(a, a.transpose(1, 2)) * self.temperature\n",
    "    \n",
    "\n",
    "    def forward(self, a, mask_a):\n",
    "        attn = self._attention(a)\n",
    "#         mask = torch.matmul(mask_a.float(), mask_a.transpose(1, 2).float()).byte()\n",
    "        mask = mask_a.byte()\n",
    "        attn.masked_fill_(~mask, -1e7)\n",
    "        attn_a = f.softmax(attn, dim=1)\n",
    "        feature_a = torch.matmul(attn_a.transpose(1, 2), a)\n",
    "        return feature_a\n",
    "\n",
    "class RE2One(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.dropout = args.dropout\n",
    "        self.blocks = nn.ModuleList([nn.ModuleDict({\n",
    "            'encoder': Encoder(args, args.embedding_dim if i == 0 else args.embedding_dim + args.hidden_size),\n",
    "            'alignment': AlignmentOne(\n",
    "                args, args.embedding_dim + args.hidden_size if i == 0 else args.embedding_dim + args.hidden_size * 2),\n",
    "            'fusion': FullFusion(\n",
    "                args, args.embedding_dim + args.hidden_size if i == 0 else args.embedding_dim + args.hidden_size * 2),\n",
    "        }) for i in range(args.blocks)])\n",
    "        self.connection = AugmentedResidual()\n",
    "        self.pooling = Pooling()\n",
    "        self.sum_pooling = SumPooling()\n",
    "    def make_mask(self, X, valid_len):\n",
    "        shape=X.shape\n",
    "        if valid_len.dim()==1:\n",
    "            valid_len=valid_len.view(-1,1).repeat(1,shape[1])\n",
    "        mask=(torch.arange(0,X.shape[1]).repeat(X.shape[0],1).to(X.device)<valid_len).float()\n",
    "        return mask.unsqueeze(2).byte()\n",
    "\n",
    "    def forward(self, a, x_len):\n",
    "        mask_a = self.make_mask(a, x_len)\n",
    "        res_a = a\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            if i > 0:\n",
    "                a = self.connection(a, res_a, i)\n",
    "                res_a = a\n",
    "            a_enc = block['encoder'](a, mask_a)\n",
    "            a = torch.cat([a, a_enc], dim=-1)\n",
    "            align_a = block['alignment'](a, mask_a)\n",
    "            a = block['fusion'](a, align_a)        \n",
    "        \n",
    "\n",
    "        hidden_max = self.pooling(a, mask_a)\n",
    "        hidden_sum = self.sum_pooling(a, mask_a)\n",
    "        hidden = torch.cat([hidden_max, hidden_sum], dim=-1)\n",
    "        \n",
    "        return hidden\n",
    "    \n",
    "class RE2Block(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.dropout = args.dropout\n",
    "        self.blocks = nn.ModuleList([nn.ModuleDict({\n",
    "            'encoder': Encoder(args, args.embedding_dim if i == 0 else args.embedding_dim + args.hidden_size),\n",
    "            'alignment': MappedAlignment(\n",
    "                args, args.embedding_dim + args.hidden_size if i == 0 else args.embedding_dim + args.hidden_size * 2),\n",
    "            'fusion': FullFusion(\n",
    "                args, args.embedding_dim + args.hidden_size if i == 0 else args.embedding_dim + args.hidden_size * 2),\n",
    "        }) for i in range(args.blocks)])\n",
    "        self.connection = AugmentedResidual()\n",
    "        self.pooling = Pooling()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def make_mask(self, X, valid_len):\n",
    "        shape=X.shape\n",
    "        if valid_len.dim()==1:\n",
    "            valid_len=valid_len.view(-1,1).repeat(1,shape[1])\n",
    "        mask=(torch.arange(0,X.shape[1]).repeat(X.shape[0],1).to(X.device)<valid_len).float()\n",
    "        return mask.unsqueeze(2).byte()\n",
    "       \n",
    "\n",
    "    def forward(self, a, b, x_len):\n",
    "        mask_a = self.make_mask(a, x_len)\n",
    "        mask_b = self.make_mask(b, x_len)\n",
    "        \n",
    "        res_a, res_b = a, b\n",
    "        \n",
    "        for i, block in enumerate(self.blocks):\n",
    "            if i > 0:\n",
    "                a = self.connection(a, res_a, i)\n",
    "                b = self.connection(b, res_b, i)\n",
    "                res_a, res_b = a, b\n",
    "            a_enc = block['encoder'](a, mask_a)\n",
    "            b_enc = block['encoder'](b, mask_b)\n",
    "            a = torch.cat([a, a_enc], dim=-1)\n",
    "            b = torch.cat([b, b_enc], dim=-1)\n",
    "            align_a, align_b = block['alignment'](a, b, mask_a, mask_b)\n",
    "            a = block['fusion'](a, align_a)\n",
    "            b = block['fusion'](b, align_b)\n",
    "        \n",
    "        \n",
    "\n",
    "        a = self.pooling(a, mask_a)\n",
    "        b = self.pooling(b, mask_b)\n",
    "        \n",
    "        hidden = torch.cat([a, b, (a - b).abs(), a * b], dim=-1) #symmetric\n",
    "        \n",
    "        return hidden\n",
    "    \n",
    "gender_weight = torch.tensor((df_train_user['gender'].value_counts().sort_index() / 3000000).values).float()\n",
    "age_weight = torch.tensor((df_train_user['age'].value_counts().sort_index() / 3000000).values).float()\n",
    "\n",
    "class RE2(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        n_dim = 100\n",
    "        self.ln_tfidf_stack = LayerNorm(55)\n",
    "        self.ln_one_hot_target = LayerNorm(72)\n",
    "\n",
    "        n_time_embedding = 16\n",
    "        self.position_embeddings = nn.Embedding(92, n_time_embedding)\n",
    "        self.click_time_embeddings = nn.Embedding(33, n_time_embedding)\n",
    "\n",
    "        self.ln_100s = nn.ModuleList([LayerNorm(100) for _ in range(4)])\n",
    "\n",
    "        \n",
    "        n_cat_hidden = 2\n",
    "        n_flatten = 55\n",
    "        n_flatten = 0\n",
    "\n",
    "        self.decoder_gender = nn.Sequential(Linear( n_cat_hidden * args.hidden_size + n_flatten, 2))\n",
    "        self.decoder_age = nn.Sequential(Linear(n_cat_hidden * args.hidden_size + n_flatten, 10))   \n",
    "        \n",
    "        self.label_smooth_gender = LabelSmoothingCrossEntropy(0.1, weights = gender_weight)\n",
    "        self.label_smooth_age = LabelSmoothingCrossEntropy(0.1, weights = age_weight)\n",
    "\n",
    "\n",
    "        self.ln_hidden = LayerNorm(n_cat_hidden * args.hidden_size)\n",
    "        self.ln_tfidf_stack = LayerNorm(55)\n",
    "        \n",
    "        self.re2_one = RE2One(args)\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                time,\n",
    "                click_time,\n",
    "                \n",
    "                ad_id_wv,\n",
    "                ad_id_glove,\n",
    "                advertiser_id_wv,\n",
    "                advertiser_id_glove,\n",
    "\n",
    "                target_encode_sequence,\n",
    "                x_len,\n",
    "                gender = None,\\\n",
    "                age = None):\n",
    "        \n",
    "        \n",
    "        time = time.to(args.device)\n",
    "        click_time = click_time.to(args.device)\n",
    "        \n",
    "        ad_id_wv = ad_id_wv.to(args.device) \n",
    "        ad_id_glove = ad_id_glove.to(args.device)\n",
    "        advertiser_id_wv = advertiser_id_wv.to(args.device)\n",
    "        advertiser_id_glove = advertiser_id_glove.to(args.device) \n",
    "\n",
    "                \n",
    "        target_encode_sequence = target_encode_sequence.to(args.device) \n",
    "        x_len = x_len.to(args.device) \n",
    "#         x_flatten = x_flatten.to(args.device)\n",
    "        \n",
    "        if gender is not None:\n",
    "            gender = gender.to(args.device) \n",
    "            age = age.to(args.device) \n",
    "    \n",
    "        \n",
    "        a_wv = torch.cat([\n",
    "            self.ln_100s[0](ad_id_wv),\n",
    "            self.ln_100s[1](ad_id_glove),\n",
    "            self.ln_100s[2](advertiser_id_wv),\n",
    "            self.ln_100s[3](advertiser_id_glove),\n",
    "            self.ln_one_hot_target(target_encode_sequence),\n",
    "            self.position_embeddings(time),\n",
    "            self.click_time_embeddings(click_time),\n",
    "        ],dim=-1)\n",
    "\n",
    "\n",
    "        hidden = self.re2_one(a_wv, x_len)\n",
    "#         cat = torch.cat([self.ln_hidden(hidden), self.ln_tfidf_stack(x_flatten)], dim = -1)\n",
    "        cat = hidden\n",
    "        output_age = self.decoder_age(cat)\n",
    "        output_gender = self.decoder_gender(cat)\n",
    "\n",
    "        if(gender is None):\n",
    "            return output_gender, output_age, hidden\n",
    "        \n",
    "        \n",
    "#         loss_age = nn.CrossEntropyLoss()\n",
    "#         loss_gender = nn.CrossEntropyLoss()\n",
    "#         l_age = loss_age(output_age,age.long())        \n",
    "#         l_gender = loss_gender(output_gender,gender.long())\n",
    "\n",
    "        l_gender = self.label_smooth_gender(output_gender, gender.long())  \n",
    "        l_age = self.label_smooth_age(output_age, age.long())\n",
    "        l=0.5*l_gender + 0.5*l_age\n",
    "            \n",
    "        \n",
    "        l=0.5*l_gender + 0.5*l_age\n",
    "        \n",
    "        \n",
    "        return l,l_gender,l_age,output_gender, output_age, hidden\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "RE2_ARG = namedtuple('ARG', [\n",
    "    'dropout',\n",
    "    'hidden_size',\n",
    "    'enc_layers',\n",
    "    'kernel_sizes',\n",
    "    'blocks',\n",
    "    'embedding_dim',\n",
    "    'device',\n",
    "])\n",
    "\n",
    "re2_args = RE2_ARG(\n",
    "    dropout = 0.3,\n",
    "    hidden_size = 256,\n",
    "    enc_layers = 2,\n",
    "    kernel_sizes = (3,),\n",
    "    blocks = 2,\n",
    "    embedding_dim = 400 + 72 + 16 + 16,\n",
    "    device = args.device,\n",
    "#       device = torch.device('cpu')\n",
    ")\n",
    "\n",
    "# model = RE2(re2_args)\n",
    "# print(sum(param.numel() for param in model.parameters()))\n",
    "\n",
    "logging.info('start training ')\n",
    "# model, val_ret_dict, test_ret_dict = nn_cross_validation_multi_task(sub_train_user, sub_train_gender, sub_train_age,\\\n",
    "#                                                                     sub_test_user, \\\n",
    "#                                                                     RE2, {'args' : re2_args}, train_multi_task, False)\n",
    "score_gender_val, score_age_val = nn_cross_validation_multi_task(sub_train_user, sub_train_gender, sub_train_age,\\\n",
    "                                                                    sub_test_user, \\\n",
    "                                                                    RE2, {'args' : re2_args}, train_multi_task, True)\n",
    "\n",
    "pickle.dump(score_gender_val, open('%s/score_gender_val.pickle' % args.save_path, 'wb'))\n",
    "pickle.dump(score_age_val, open('%s/score_age_val.pickle' % args.save_path, 'wb'))\n",
    "# np.save('%s/hidden_val.pickle' % args.save_path, hidden_val)\n",
    "\n",
    "logging.info('finish training ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(hyr)\n",
   "language": "python",
   "name": "hyr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
