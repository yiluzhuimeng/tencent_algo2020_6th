{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangweilin/anaconda3/envs/fjw/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/huangweilin/anaconda3/envs/fjw/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/huangweilin/anaconda3/envs/fjw/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/huangweilin/anaconda3/envs/fjw/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/huangweilin/anaconda3/envs/fjw/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/huangweilin/anaconda3/envs/fjw/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import   tqdm_notebook as tqdm\n",
    "import _pickle as pk\n",
    "import os\n",
    "import json\n",
    "from IPython.display import display,HTML\n",
    "from category_encoders import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import *\n",
    "from transformers.modeling_bert import BertConfig,BertLayerNorm\n",
    "from transformers.activations import gelu, gelu_new, swish\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from torch.nn.utils.rnn import PackedSequence\n",
    "from torchcontrib.optim import SWA\n",
    "np.random.seed(13)\n",
    "import collections\n",
    "import math\n",
    "import time\n",
    "import logging\n",
    "import copy\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO \n",
    ")\n",
    "ARG=collections.namedtuple('ARG',['train_batch_size',\n",
    " 'eval_batch_size',\n",
    " 'weight_decay',\n",
    " 'learning_rate',\n",
    " 'adam_epsilon',\n",
    " 'num_train_epochs',\n",
    " 'warmup_steps',\n",
    " 'gradient_accumulation_steps',\n",
    " 'save_steps',\n",
    " 'max_grad_norm',\n",
    " 'model_name_or_path',\n",
    " 'output_dir',\n",
    " 'seed',\n",
    " 'device',\n",
    " 'n_gpu',\n",
    " 'max_steps',\n",
    " 'output_mode',\n",
    "'fp16_opt_level',\n",
    "'fp16',\n",
    "'card_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features=pk.load(open(\"../../var/fjw/train_mid/new_feature_simple.pk\",\"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_click_log_time=pk.load(open(\"../../var/fjw/usr_seq/se_user_time.pickle\",\"rb\"))\n",
    "full_click_log_creative_id=pk.load(open(\"../../var/fjw/usr_seq/se_user_creative_id.pickle\",\"rb\"))\n",
    "# full_click_log_creative_id=pk.load(open(\"./dataset/usr_seq/se_user_creative_id_shuffle.pk\",\"rb\"))\n",
    "full_click_log_click_times=pk.load(open(\"../../var/fjw/usr_seq/se_user_click_times.pickle\",\"rb\"))\n",
    "# full_click_log_click_times=pk.load(open(\"./dataset/usr_seq/se_user_click_time_shuffle.pk\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_dict,target_map_dicts=pk.load(open(\"../../var/fjw/train_mid/new_target_info_simple.pk\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_info=pk.load(open(\"../../var/fjw/se_tfidf_stack_new.pickle\",\"rb\"))\n",
    "cal_series=pk.load(open(\"../../var/fjw/simple_cal_norm.csv\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model=np.load(\"../../var/fjw/glove_model_min_dim50_full.npy\")\n",
    "#./dataset/id_embedding.npy\n",
    "#./dataset/glove_model_min_dim50_full.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangweilin/anaconda3/envs/fjw/lib/python3.6/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f668c48165c40dab82f2e6ec7df5cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "label_info=np.array(train_features.values.tolist())\n",
    "age_nums=[0]*10\n",
    "gender_nums=[0]*2\n",
    "for idx in tqdm(range(label_info.shape[0])):\n",
    "    age_nums[label_info[idx,0]-1]+=1\n",
    "    gender_nums[label_info[idx,1]-1]+=1\n",
    "age_nums=np.array(age_nums)/sum(age_nums)\n",
    "gender_nums=np.array(gender_nums)/sum(gender_nums)\n",
    "age_nums=torch.tensor(age_nums).float()\n",
    "gender_nums=torch.tensor(gender_nums).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0,
     8
    ]
   },
   "outputs": [],
   "source": [
    "class UsrDataset(Data.Dataset):\n",
    "    def __init__(self,examples):\n",
    "        self.usr_id=examples.index.tolist()\n",
    "    def __len__(self):\n",
    "        return len(self.usr_id)\n",
    "    def __getitem__(self,idx):\n",
    "        return  self.usr_id[idx]\n",
    "\n",
    "def collate_fn(usr_ids):\n",
    "    max_len=128\n",
    "        \n",
    "    train_data=train_features[usr_ids]\n",
    "    lengths=[]\n",
    "    times,creative_ids,click_times,ad_ids,product_ids,product_categorys,\\\n",
    "    advertiser_ids,industrys,target_encodings,cals,tfidfs,ages,genders= [],[],[],[],[],[],[],[],[],[],[],[],[]\n",
    "    advertiser_id_industrys,\\\n",
    "    product_cat_advertisers,product_cat_industrys,\\\n",
    "    product_id_product_cats,product_id_advertisers,product_id_industrys=[],[],[],\\\n",
    "                                                                            [],[],[]\n",
    "    tfidf_sequences=[]\n",
    "    textrank_sequences=[]\n",
    "    one_hot_targets=[]\n",
    "    graph_sequences=[]\n",
    "    target_encode_aggs=torch.zeros((len(usr_ids),360)).float()\n",
    "    for idx,(usr_id,sample) in enumerate(zip(usr_ids,train_data)):\n",
    "        if sample[0]>0:\n",
    "            age,gender=torch.tensor(sample[0]).long(),torch.tensor(sample[1]).long()\n",
    "        else:\n",
    "            age,gender=torch.tensor(sample[0]+1).long(),torch.tensor(sample[1]+1).long()\n",
    "        ages.append(age)\n",
    "        genders.append(gender)\n",
    "#         creative_idx=pk.load(open(\"./dataset/usr_id/%d\"%usr_id,\"rb\"))\n",
    "        fold=usr_dict[usr_id]\n",
    "        creative_idx=full_click_log_creative_id.at[usr_id]\n",
    "        creative_id_list=[str(e) for e in creative_idx]\n",
    "        time_list=full_click_log_time.at[usr_id]\n",
    "        times.append(torch.tensor(time_list[:max_len]).long()-1)\n",
    "        click_times_list=list(map(lambda x:x if x<=31 else 32,full_click_log_click_times.at[usr_id]))\n",
    "        click_times.append(torch.tensor(click_times_list[:max_len]).long()-1)\n",
    "#         features=np.load(open(\"./dataset/usr_feature_simple/%d.npy\"%usr_id,\"rb\"))\n",
    "        one_hot_target=np.stack(target_map_dicts[fold].loc[creative_id_list].values)\n",
    "        features=one_hot_target\n",
    "        length=len(creative_idx) if len(creative_idx)<max_len else max_len\n",
    "        lengths.append(length)\n",
    "        word2vec=w2v_model[creative_idx]\n",
    "        cal=cal_series.at[usr_id]\n",
    "        cals.append(torch.tensor(cal).float())\n",
    "        tfidf=tfidf_info.at[usr_id]\n",
    "        tfidfs.append(torch.tensor(tfidf).float())\n",
    "#         target_encode_aggs[idx]=torch.tensor(target_encode_agg[usr_id]).float()\n",
    "#         target_encoding=torch.tensor(features[1][:max_len,:12]).float()\n",
    "#         target_encodings.append(target_encoding)\n",
    "#         graph_info=torch.tensor(features[:max_len,:28]).float()\n",
    "#         graph_sequences.append(graph_info)\n",
    "#         tr_info=torch.tensor(textrank_sequence_info.at[usr_id][:max_len]).float()\n",
    "#         textrank_sequences.append(tr_info)\n",
    "#         tfidf_sequence=torch.tensor(tfidf_sequence_info.at[usr_id][:max_len]).float()\n",
    "#         tfidf_sequences.append(tfidf_sequence)\n",
    "        one_hot_target=torch.tensor(features[:max_len,:]).float()\n",
    "        one_hot_targets.append(one_hot_target)\n",
    "        \n",
    "#         creative_id=word2vec[:max_len,list(range(200))]\n",
    "#         creative_ids.append(torch.tensor(creative_id).float())\n",
    "\n",
    "#         ad_id=word2vec[:max_len,list(range(200,400))]\n",
    "#         ad_ids.append(torch.tensor(ad_id).float())\n",
    "\n",
    "#         product_id=word2vec[:max_len,list(range(400,500))]\n",
    "#         product_ids.append(torch.tensor(product_id).float())\n",
    "\n",
    "#         product_category=word2vec[:max_len,list(range(500,600))]\n",
    "#         product_categorys.append(torch.tensor(product_category).float())\n",
    "\n",
    "#         advertiser_id=word2vec[:max_len,list(range(600,800))]\n",
    "#         advertiser_ids.append(torch.tensor(advertiser_id).float())\n",
    "\n",
    "#         industry=word2vec[:max_len,list(range(800,900))]\n",
    "#         industrys.append(torch.tensor(industry).float())\n",
    "    \n",
    "#         product_cat_industry=word2vec[:max_len,list(range(1000,1100))]\n",
    "#         product_cat_industrys.append(torch.tensor(product_cat_industry).float())\n",
    "#         product_id_advertiser=word2vec[:max_len,list(range(900,1000))]\n",
    "#         product_id_advertisers.append(torch.tensor(product_id_advertiser).float())\n",
    "  \n",
    "        creative_id=word2vec[:max_len,list(range(50))]\n",
    "        creative_ids.append(torch.tensor(creative_id).float())\n",
    "\n",
    "        ad_id=word2vec[:max_len,list(range(50,100))]\n",
    "        ad_ids.append(torch.tensor(ad_id).float())\n",
    "\n",
    "        product_id=word2vec[:max_len,list(range(100,150))]\n",
    "        product_ids.append(torch.tensor(product_id).float())\n",
    "\n",
    "        product_category=word2vec[:max_len,list(range(150,200))]\n",
    "        product_categorys.append(torch.tensor(product_category).float())\n",
    "\n",
    "        advertiser_id=word2vec[:max_len,list(range(200,250))]\n",
    "        advertiser_ids.append(torch.tensor(advertiser_id).float())\n",
    "\n",
    "        industry=word2vec[:max_len,list(range(250,300))]\n",
    "        industrys.append(torch.tensor(industry).float())\n",
    "    \n",
    "#         advertiser_id_industry=word2vec[:max_len,list(range(300,350))]\n",
    "#         advertiser_id_industrys.append(torch.tensor(advertiser_id_industry).float())\n",
    "#         product_cat_advertiser=word2vec[:max_len,list(range(350,400))]\n",
    "#         product_cat_advertisers.append(torch.tensor(product_cat_advertiser).float())\n",
    "        product_cat_industry=word2vec[:max_len,list(range(400,450))]\n",
    "        product_cat_industrys.append(torch.tensor(product_cat_industry).float())\n",
    "        product_id_advertiser=word2vec[:max_len,list(range(450,500))]\n",
    "        product_id_advertisers.append(torch.tensor(product_id_advertiser).float())\n",
    "#         product_id_industry=word2vec[:max_len,list(range(500,550))]\n",
    "#         product_id_industrys.append(torch.tensor(product_id_industry).float())\n",
    "#         product_id_product_cat=word2vec[:max_len,list(range(550,600))]\n",
    "#         product_id_product_cats.append(torch.tensor(product_id_product_cat).float())\n",
    "\n",
    "    times=torch.nn.utils.rnn.pad_sequence(times,padding_value=0, batch_first=True)\n",
    "    creative_ids=torch.nn.utils.rnn.pad_sequence(creative_ids,padding_value=0, batch_first=True)\n",
    "    click_times=torch.nn.utils.rnn.pad_sequence(click_times,padding_value=0, batch_first=True)\n",
    "    ad_ids=torch.nn.utils.rnn.pad_sequence(ad_ids,padding_value=0, batch_first=True)\n",
    "    product_ids=torch.nn.utils.rnn.pad_sequence(product_ids,padding_value=0,batch_first=True)\n",
    "    product_categorys=torch.nn.utils.rnn.pad_sequence(product_categorys,padding_value=0,batch_first=True)\n",
    "    advertiser_ids=torch.nn.utils.rnn.pad_sequence(advertiser_ids,padding_value=0,batch_first=True)\n",
    "    industrys=torch.nn.utils.rnn.pad_sequence(industrys,padding_value=0,batch_first=True)\n",
    "#     advertiser_id_industrys=torch.nn.utils.rnn.pad_sequence(advertiser_id_industrys,padding_value=0,batch_first=True)\n",
    "#     product_cat_advertisers=torch.nn.utils.rnn.pad_sequence(product_cat_advertisers,padding_value=0,batch_first=True)\n",
    "#     product_id_product_cats=torch.nn.utils.rnn.pad_sequence(product_id_product_cats,padding_value=0,batch_first=True)\n",
    "    product_cat_industrys=torch.nn.utils.rnn.pad_sequence(product_cat_industrys,padding_value=0,batch_first=True)\n",
    "    product_id_advertisers=torch.nn.utils.rnn.pad_sequence(product_id_advertisers,padding_value=0,batch_first=True)\n",
    "#     product_id_industrys=torch.nn.utils.rnn.pad_sequence(product_id_industrys,padding_value=0,batch_first=True)\n",
    "#     target_encodings=torch.nn.utils.rnn.pad_sequence(target_encodings,padding_value=0,batch_first=True)\n",
    "#     tfidf_sequences=torch.nn.utils.rnn.pad_sequence(tfidf_sequences,padding_value=0,batch_first=True)\n",
    "#     textrank_sequences=torch.nn.utils.rnn.pad_sequence(textrank_sequences,padding_value=0,batch_first=True)\n",
    "    one_hot_targets=torch.nn.utils.rnn.pad_sequence(one_hot_targets,padding_value=0,batch_first=True)\n",
    "#     graph_sequences=torch.nn.utils.rnn.pad_sequence(graph_sequences,padding_value=0,batch_first=True)\n",
    "#     target_embed_info=one_hot_targets*torch.cat([torch.arange(0,10),torch.arange(0,2)]).repeat(6).float().unsqueeze(0).unsqueeze(0)\n",
    "#     target_embed_info=[(e.sum(dim=-1)//0.5).long() if idx%2==0  else (e.sum(dim=-1)//0.1).long()\\\n",
    "#                        for idx,e in enumerate(list(torch.split(target_embed_info,\\\n",
    "#                                                                              [10,2,10,2,10,2,10,2,10,2,10,2],dim=-1)))]\n",
    "    target_embed_info=[torch.zeros(1)]                                                                    \n",
    "    return torch.tensor(lengths).long(),times,click_times,\\\n",
    "            creative_ids,ad_ids,product_ids,product_categorys,advertiser_ids, industrys,\\\n",
    "            target_encodings,product_cat_industrys,product_id_advertisers,product_id_industrys,\\\n",
    "            torch.stack(cals).float(),torch.stack(tfidfs).float(),tfidf_sequences,one_hot_targets,\\\n",
    "            textrank_sequences,graph_sequences,\\\n",
    "            advertiser_id_industrys,product_cat_advertisers,\\\n",
    "            product_id_product_cats,target_encode_aggs,\\\n",
    "            torch.stack(ages).long()-1,torch.stack(genders).long()-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=UsrDataset(train_features)\n",
    "train_dataloader=Data.DataLoader(train_dataset,batch_size=256,shuffle=False,\\\n",
    "                                 collate_fn=collate_fn,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0,
     3,
     82,
     94,
     139,
     152,
     164,
     198
    ]
   },
   "outputs": [],
   "source": [
    "def mish(x):\n",
    "    return x * torch.tanh(nn.functional.softplus(x))\n",
    "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish, \"gelu_new\": gelu_new, \"mish\": mish}\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n",
    "            )\n",
    "        self.output_attentions = config.output_attentions\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        click_times=None,\n",
    "    ):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "\n",
    "        # If this is instantiated as a cross-attention module, the keys\n",
    "        # and values come from an encoder; the attention mask needs to be\n",
    "        # such that the encoder's padding tokens are not attended to.\n",
    "        if encoder_hidden_states is not None:\n",
    "            mixed_key_layer = self.key(encoder_hidden_states)\n",
    "            mixed_value_layer = self.value(encoder_hidden_states)\n",
    "            attention_mask = encoder_attention_mask\n",
    "        else:\n",
    "            mixed_key_layer = self.key(hidden_states)\n",
    "            mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if click_times is not None:\n",
    "            attention_scores=attention_scores*click_times.unsqueeze(1).unsqueeze(1).float()\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)\n",
    "        return outputs\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        mask = torch.ones(self.self.num_attention_heads, self.self.attention_head_size)\n",
    "        heads = set(heads) - self.pruned_heads  # Convert to set and remove already pruned heads\n",
    "        for head in heads:\n",
    "            # Compute how many pruned heads are before the head and move the index accordingly\n",
    "            head = head - sum(1 if h < head else 0 for h in self.pruned_heads)\n",
    "            mask[head] = 0\n",
    "        mask = mask.view(-1).contiguous().eq(1)\n",
    "        index = torch.arange(len(mask))[mask].long()\n",
    "\n",
    "        # Prune linear layers\n",
    "        self.self.query = prune_linear_layer(self.self.query, index)\n",
    "        self.self.key = prune_linear_layer(self.self.key, index)\n",
    "        self.self.value = prune_linear_layer(self.self.value, index)\n",
    "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
    "\n",
    "        # Update hyper params and store pruned heads\n",
    "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
    "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        click_times=None,\n",
    "    ):\n",
    "        self_outputs = self.self(\n",
    "            hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask,click_times\n",
    "        )\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        return outputs\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = BertAttention(config)\n",
    "        self.is_decoder = config.is_decoder\n",
    "        if self.is_decoder:\n",
    "            self.crossattention = BertAttention(config)\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        click_times=None,\n",
    "    ):\n",
    "        self_attention_outputs = self.attention(hidden_states, attention_mask, head_mask,click_times=click_times)\n",
    "        attention_output = self_attention_outputs[0]\n",
    "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        if self.is_decoder and encoder_hidden_states is not None:\n",
    "            cross_attention_outputs = self.crossattention(\n",
    "                attention_output, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask\n",
    "            )\n",
    "            attention_output = cross_attention_outputs[0]\n",
    "            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n",
    "\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        outputs = (layer_output,) + outputs\n",
    "        return outputs\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        click_times=None,\n",
    "    ):\n",
    "        all_hidden_states = ()\n",
    "        all_attentions = ()\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if self.output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_outputs = layer_module(\n",
    "                hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask,click_times=click_times\n",
    "            )\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if self.output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        # Add last layer\n",
    "        if self.output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "        if self.output_hidden_states:\n",
    "            outputs = outputs + (all_hidden_states,)\n",
    "        if self.output_attentions:\n",
    "            outputs = outputs + (all_attentions,)\n",
    "        return outputs  # last-layer hidden state, (all hidden states), (all attentions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": [
     0,
     21,
     35,
     57,
     74,
     84,
     131,
     212,
     234,
     255,
     431
    ]
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, smoothing=0.1,weights=torch.ones(2)/2):\n",
    "\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        assert smoothing < 1.0\n",
    "        self.smoothing = smoothing\n",
    "        self.confidence = 1. - smoothing\n",
    "        self.weights=weights.to(device)\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        logprobs = F.log_softmax(x, dim=-1)\n",
    "#         new_target=torch.zeros(X.shape).scatter_(1,target.unsqueeze(1),1)\n",
    "#         smooth_target=new_target*0.9+torch.ones_like(new_target)*(0.1/new_target.shape[1])\n",
    "#         -(F.log_softmax(X,dim=-1)*smooth_target).sum(dim=-1).mean()\n",
    "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = (-logprobs*self.weights).sum(dim=-1)\n",
    "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
    "        return loss.mean()\n",
    "def masked_softmax(X,valid_len):\n",
    "    if valid_len is None:\n",
    "        return F.softmax(X,dim=-1)\n",
    "    else:\n",
    "        shape=X.shape\n",
    "        if valid_len.dim()==1:\n",
    "            valid_len=valid_len.view(-1,1).repeat(1,shape[1]).view(-1)\n",
    "        else:\n",
    "            valid_len=valid_len.view(-1)\n",
    "        X=X.view(-1,shape[-1])    \n",
    "        mask=(torch.arange(0,X.shape[-1]).repeat(X.shape[0],1).float().to(X.device)<valid_len.view(-1,1).repeat(1,X.shape[-1]).float()).float()\n",
    "        mask=torch.log(mask)\n",
    "        X=X+mask\n",
    "        return F.softmax(X,dim=-1).view(shape)\n",
    "class RNNDropout(nn.Dropout):\n",
    "    \"\"\"\n",
    "    Dropout layer for the inputs of RNNs.\n",
    "    Apply the same dropout mask to all the elements of the same sequence in\n",
    "    a batch of sequences of size (batch, sequences_length, embedding_dim).\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, sequences_batch):\n",
    "        \"\"\"\n",
    "        Apply dropout to the input batch of sequences.\n",
    "        Args:\n",
    "            sequences_batch: A batch of sequences of vectors that will serve\n",
    "                as input to an RNN.\n",
    "                Tensor of size (batch, sequences_length, emebdding_dim).\n",
    "        Returns:\n",
    "            A new tensor on which dropout has been applied.\n",
    "        \"\"\"\n",
    "        ones = sequences_batch.data.new_ones(sequences_batch.shape[0],\n",
    "                                             sequences_batch.shape[-1])\n",
    "        dropout_mask = nn.functional.dropout(ones, self.p, self.training,\n",
    "                                             inplace=False)\n",
    "        return dropout_mask.unsqueeze(1) * sequences_batch\n",
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self,dropout=0):\n",
    "        super(DotProductAttention,self).__init__()\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    # query: (batch_size, #queries, d)\n",
    "    # key: (batch_size, #kv_pairs, d)\n",
    "    # value: (batch_size, #kv_pairs, dim_v)\n",
    "    # valid_len: either (batch_size, ) or (batch_size, xx)\n",
    "    def forward(self,query,key,value,valid_len=None):\n",
    "        d=query.shape[-1]\n",
    "        shape=query.shape\n",
    "        if valid_len.dim()==1:\n",
    "            valid_len=valid_len.view(-1,1).repeat(1,shape[1])\n",
    "        mask=(torch.arange(0,query.shape[1]).repeat(query.shape[0],1).to(query.device)<valid_len).float()\n",
    "        scores=torch.bmm(query,key.permute(0,2,1))/math.sqrt(d)\n",
    "        attention_weights=self.dropout(masked_softmax(scores,valid_len))\n",
    "        return torch.bmm(attention_weights,value)*mask.unsqueeze(-1)\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,features,eps=1e-6):\n",
    "        super(LayerNorm,self).__init__()\n",
    "        self.gamma=nn.Parameter(torch.ones(features))\n",
    "        self.beta=nn.Parameter(torch.zeros(features))\n",
    "        self.eps=eps\n",
    "    def forward(self,X):\n",
    "        mean=X.mean(-1,keepdim=True)\n",
    "        std=X.std(-1,keepdim=True)\n",
    "        return self.gamma*(X-mean)/(std+self.eps)+self.beta\n",
    "class TransEncoder(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(TransEncoder,self).__init__()\n",
    "        self.Encoder=BertEncoder(config=config)\n",
    "        self.P=PositionalEncoding(config)\n",
    "        self.config=config\n",
    "        for n,e in self.Encoder.named_modules():\n",
    "            self._init_weights(e)\n",
    "        for n,e in self.P.named_modules():\n",
    "            self._init_weights(e)\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" Initialize the weights \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        elif isinstance(module, BertLayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "    def make_mask(self,X,valid_len):\n",
    "            shape=X.shape\n",
    "            if valid_len.dim()==1:\n",
    "                valid_len=valid_len.view(-1,1).repeat(1,shape[1])\n",
    "            mask=(torch.arange(0,X.shape[1]).repeat(X.shape[0],1).to(X.device)<valid_len).float()\n",
    "            return mask\n",
    "    def forward(self,X,length):\n",
    "        #make attention mask\n",
    "        attention_mask=self.make_mask(X,length)\n",
    "        embedding_output=self.P(X)\n",
    "        #adjust attention mask\n",
    "        if attention_mask.dim() == 3:\n",
    "            extended_attention_mask = attention_mask[:, None, :, :]\n",
    "        elif attention_mask.dim() == 2:\n",
    "            extended_attention_mask = attention_mask[:, None, None, :]\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        \n",
    "        #make head mask\n",
    "        head_mask = [None] * self.config.num_hidden_layers\n",
    "        outputs=self.Encoder(  embedding_output,\n",
    "                attention_mask=extended_attention_mask,\n",
    "                head_mask=head_mask,\n",
    "                encoder_hidden_states=None,\n",
    "                encoder_attention_mask=None,)\n",
    "        return outputs[0]\n",
    "class Seq2SeqEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN taking variable length padded sequences of vectors as input and\n",
    "    encoding them into padded sequences of vectors of the same length.\n",
    "    This module is useful to handle batches of padded sequences of vectors\n",
    "    that have different lengths and that need to be passed through a RNN.\n",
    "    The sequences are sorted in descending order of their lengths, packed,\n",
    "    passed through the RNN, and the resulting sequences are then padded and\n",
    "    permuted back to the original order of the input sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 rnn_type,\n",
    "                 input_size,\n",
    "                 hidden_size,\n",
    "                 num_layers=1,\n",
    "                 bias=True,\n",
    "                 dropout=0.0,\n",
    "                 bidirectional=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rnn_type: The type of RNN to use as encoder in the module.\n",
    "                Must be a class inheriting from torch.nn.RNNBase\n",
    "                (such as torch.nn.LSTM for example).\n",
    "            input_size: The number of expected features in the input of the\n",
    "                module.\n",
    "            hidden_size: The number of features in the hidden state of the RNN\n",
    "                used as encoder by the module.\n",
    "            num_layers: The number of recurrent layers in the encoder of the\n",
    "                module. Defaults to 1.\n",
    "            bias: If False, the encoder does not use bias weights b_ih and\n",
    "                b_hh. Defaults to True.\n",
    "            dropout: If non-zero, introduces a dropout layer on the outputs\n",
    "                of each layer of the encoder except the last one, with dropout\n",
    "                probability equal to 'dropout'. Defaults to 0.0.\n",
    "            bidirectional: If True, the encoder of the module is bidirectional.\n",
    "                Defaults to False.\n",
    "        \"\"\"\n",
    "        assert issubclass(rnn_type, nn.RNNBase),            \"rnn_type must be a class inheriting from torch.nn.RNNBase\"\n",
    "\n",
    "        super(Seq2SeqEncoder, self).__init__()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self._encoder = rnn_type(input_size,\n",
    "                                 hidden_size,\n",
    "                                 num_layers=num_layers,\n",
    "                                 bias=bias,\n",
    "                                 batch_first=True,\n",
    "                                 dropout=dropout,\n",
    "                                 bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, sequences_batch, sequences_lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequences_batch: A batch of variable length sequences of vectors.\n",
    "                The batch is assumed to be of size\n",
    "                (batch, sequence, vector_dim).\n",
    "            sequences_lengths: A 1D tensor containing the sizes of the\n",
    "                sequences in the input batch.\n",
    "        Returns:\n",
    "            reordered_outputs: The outputs (hidden states) of the encoder for\n",
    "                the sequences in the input batch, in the same order.\n",
    "        \"\"\"\n",
    "        total_length=sequences_batch.shape[1]\n",
    "        packed_batch = nn.utils.rnn.pack_padded_sequence(sequences_batch,\n",
    "                                                         sequences_lengths,\n",
    "                                                         batch_first=True,enforce_sorted=False)\n",
    "\n",
    "        outputs, _ = self._encoder(packed_batch, None)\n",
    "\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs,\n",
    "                                                      batch_first=True,total_length=total_length)\n",
    "\n",
    "        return outputs\n",
    "def _init_esim_weights(module):\n",
    "    \"\"\"\n",
    "    Initialise the weights of the ESIM model.\n",
    "    \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.xavier_uniform_(module.weight.data)\n",
    "        nn.init.constant_(module.bias.data, 0.0)\n",
    "\n",
    "    elif isinstance(module, nn.LSTM):\n",
    "        nn.init.xavier_uniform_(module.weight_ih_l0.data)\n",
    "        nn.init.orthogonal_(module.weight_hh_l0.data)\n",
    "        nn.init.constant_(module.bias_ih_l0.data, 0.0)\n",
    "        nn.init.constant_(module.bias_hh_l0.data, 0.0)\n",
    "        hidden_size = module.bias_hh_l0.data.shape[0] // 4\n",
    "        module.bias_hh_l0.data[hidden_size:(2*hidden_size)] = 1.0\n",
    "\n",
    "        if (module.bidirectional):\n",
    "            nn.init.xavier_uniform_(module.weight_ih_l0_reverse.data)\n",
    "            nn.init.orthogonal_(module.weight_hh_l0_reverse.data)\n",
    "            nn.init.constant_(module.bias_ih_l0_reverse.data, 0.0)\n",
    "            nn.init.constant_(module.bias_hh_l0_reverse.data, 0.0)\n",
    "            module.bias_hh_l0_reverse.data[hidden_size:(2*hidden_size)] = 1.0\n",
    "class Conditional_LayerNorm(nn.Module):\n",
    "    def __init__(self,features,conditional_dim,eps=1e-6):\n",
    "        super(Conditional_LayerNorm,self).__init__()\n",
    "        self.gamma=nn.Parameter(torch.ones(features))\n",
    "        self.beta=nn.Parameter(torch.zeros(features))\n",
    "        self.trans_gamma=nn.Linear(conditional_dim,self.gamma.shape[-1])\n",
    "        self.trans_beta=nn.Linear(conditional_dim,self.beta.shape[-1])\n",
    "        torch.nn.init.constant_(self.trans_gamma.weight,val=0)\n",
    "        torch.nn.init.constant_(self.trans_gamma.bias,val=0)\n",
    "        torch.nn.init.constant_(self.trans_beta.weight,val=0)\n",
    "        torch.nn.init.constant_(self.trans_beta.bias,val=0)\n",
    "        self.eps=eps\n",
    "    def forward(self,X,condition):\n",
    "        mean=X.mean(-1,keepdim=True)\n",
    "        std=X.std(-1,keepdim=True)\n",
    "        cond_gamma=self.trans_gamma(condition)\n",
    "        cond_beta=self.trans_beta(condition)\n",
    "        if condition.dim()<X.dim(): #condition是固定维度\n",
    "            return (self.gamma+cond_gamma).unsqueeze(1)*(X-mean)/(std+self.eps)+(self.beta+cond_beta).unsqueeze(1)\n",
    "        else:#condition是sequence\n",
    "            return (self.gamma+cond_gamma)*(X-mean)/(std+self.eps)+(self.beta+cond_beta)\n",
    "class ESIM(nn.Module):\n",
    "\n",
    "    def __init__(self,config,\n",
    "                 dropout=0,\n",
    "                 device=\"cpu\"):\n",
    "        super(ESIM, self).__init__()\n",
    "        embedding_dim=config.hidden_size\n",
    "        hidden_size=config.hidden_size\n",
    "        self.embedding_dim=embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        if self.dropout:\n",
    "            self._rnn_dropout = RNNDropout(p=self.dropout)\n",
    "\n",
    "\n",
    "#         self._encoding=TransEncoder(config)\n",
    "        self._encoding_h = Seq2SeqEncoder(nn.LSTM,\n",
    "                                        self.embedding_dim,\n",
    "                                        self.hidden_size,\n",
    "                                        bidirectional=True)\n",
    "        self.cond_h=Conditional_LayerNorm(self.hidden_size*2,50)\n",
    "        self.cond_p=Conditional_LayerNorm(self.hidden_size*2,50)\n",
    "#         self._encoding_p = Seq2SeqEncoder(nn.LSTM,\n",
    "#                                         self.embedding_dim,\n",
    "#                                         self.hidden_size,\n",
    "#                                         bidirectional=True)\n",
    "\n",
    "        self._attention =DotProductAttention()\n",
    "\n",
    "        self._projection = nn.Sequential(nn.Linear(2*4*self.hidden_size,\n",
    "                                                   self.hidden_size),\n",
    "                                         nn.ReLU())\n",
    "        self.time_embeddings = nn.Embedding(config.max_position_embeddings, 32)\n",
    "        self.click_times_embeddings = nn.Embedding(32, 32)\n",
    "        self.ad_age_embeddings = nn.Embedding(24, 32)\n",
    "        self.ad_gender_embeddings = nn.Embedding(24, 32)\n",
    "        self.advertiser_age_embeddings = nn.Embedding(24, 32)\n",
    "        self.advertiser_gender_embeddings = nn.Embedding(24, 32)\n",
    "\n",
    "        self._composition = Seq2SeqEncoder(nn.LSTM,\n",
    "                                           self.hidden_size*2,\n",
    "                                           self.hidden_size,\n",
    "                                           bidirectional=True)\n",
    "        self.ln_create=LayerNorm(50)\n",
    "        self.ln_ad=LayerNorm(50)\n",
    "        self.ln_product_id=LayerNorm(50)\n",
    "        self.ln_product_cat=LayerNorm(50)\n",
    "        self.ln_advertiser=LayerNorm(50)\n",
    "        self.ln_industry=LayerNorm(50)\n",
    "        \n",
    "        \n",
    "        self.ln_advertiser_id_industry=LayerNorm(50)\n",
    "        self.ln_product_cat_industry=LayerNorm(50)\n",
    "        self.ln_product_cat_advertiser=LayerNorm(50)\n",
    "        self.ln_product_id_advertiser=LayerNorm(50)\n",
    "        self.ln_product_id_industry=LayerNorm(50)\n",
    "        self.ln_product_id_product_cat=LayerNorm(50)\n",
    "\n",
    "\n",
    "        \n",
    "        self.ln_time=LayerNorm(32)\n",
    "        self.ln_click_times=LayerNorm(32)\n",
    "        self.ln_ad_age=LayerNorm(32)\n",
    "        self.ln_ad_gender=LayerNorm(32)\n",
    "        self.ln_advertiser_age=LayerNorm(32)\n",
    "        self.ln_advertiser_gender=LayerNorm(32)\n",
    "        \n",
    "        \n",
    "        self.ln_target_enc=LayerNorm(12)\n",
    "        self.ln_tfidf=LayerNorm(55)\n",
    "        self.ln_graph=LayerNorm(42)\n",
    "        self.ln_cal=LayerNorm(31)\n",
    "        self.ln_one_hot_target=LayerNorm(72)\n",
    "        self.ln_tfidf_sequence=LayerNorm(6)\n",
    "        self.ln_tr_sequence=LayerNorm(6)\n",
    "        self.decoder_gender=nn.Sequential(nn.Dropout(p=self.dropout),\n",
    "                                             nn.Linear(2*4*self.hidden_size+55,\n",
    "                                                       self.hidden_size),\n",
    "                                             nn.Tanh(),\n",
    "                                             nn.Dropout(p=self.dropout),\n",
    "                                             nn.Linear(self.hidden_size,\n",
    "                                                       2))\n",
    "        self.decoder_age=nn.Sequential(nn.Dropout(p=self.dropout),\n",
    "                                             nn.Linear(2*4*self.hidden_size+55,\n",
    "                                                       self.hidden_size),\n",
    "                                             nn.Tanh(),\n",
    "                                             nn.Dropout(p=self.dropout),\n",
    "                                             nn.Linear(self.hidden_size,\n",
    "                                                       10))\n",
    "\n",
    "        # Initialize all weights and biases in the model.\n",
    "        self.apply(_init_esim_weights)\n",
    "    def make_mask(self,X,valid_len):\n",
    "            shape=X.shape\n",
    "            if valid_len.dim()==1:\n",
    "                valid_len=valid_len.view(-1,1).repeat(1,shape[1])\n",
    "            mask=(torch.arange(0,X.shape[1]).repeat(X.shape[0],1).to(X.device)<valid_len).float()\n",
    "            return mask\n",
    "    def replace_masked(self,tensor, mask, value):\n",
    "        mask = mask.unsqueeze(1).transpose(2, 1)\n",
    "        reverse_mask = 1.0 - mask\n",
    "        values_to_add = value * reverse_mask\n",
    "        return tensor * mask + values_to_add\n",
    "    def forward(self,times,click_times,create_embeddings,ad_embeddings,product_id_embeddings,                product_cat_embeddings,advertiser_embeddings,industry_embeddings,target_encodings,                product_cat_industry_embeddings,product_cat_advertisers,product_id_advertiser_embeddings,                product_id_industry_embeddings,product_id_product_cats,                advertiser_id_industrys,target_embed_info,                length,cal,tfidf,tfidf_sequence,one_hot_target,tr_sequence,graph_sequence,                age,gender,encoder_hidden_states=None,encoder_extended_attention_mask=None):\n",
    "        norm_time=self.ln_time(self.time_embeddings(times))\n",
    "        norm_click_time=self.click_times_embeddings(click_times)\n",
    "        norm_target=self.ln_one_hot_target(one_hot_target)\n",
    "        premises=torch.cat([self.ln_create(create_embeddings[:,:,:]),                            self.ln_ad(ad_embeddings[:,:,:]),                            self.ln_product_id(product_id_embeddings),                            norm_target,norm_time,                            norm_click_time],dim=-1)\n",
    "        hypotheses=torch.cat([self.ln_advertiser(advertiser_embeddings[:,:,:]),                            self.ln_product_cat_industry(product_cat_industry_embeddings[:,:,:]),                            self.ln_product_id_advertiser(product_id_advertiser_embeddings[:,:,:]),\n",
    "                            norm_target,norm_time,\\\n",
    "                            norm_click_time],dim=-1)\n",
    "        premises_lengths=length\n",
    "        hypotheses_lengths=premises_lengths\n",
    "        premises_mask = self.make_mask(premises, premises_lengths).to(self.device)\n",
    "        hypotheses_mask = self.make_mask(hypotheses, hypotheses_lengths).to(self.device)\n",
    "\n",
    "        embedded_premises = premises\n",
    "        embedded_hypotheses = hypotheses\n",
    "\n",
    "        if self.dropout:\n",
    "            embedded_premises = self._rnn_dropout(embedded_premises)\n",
    "            embedded_hypotheses = self._rnn_dropout(embedded_hypotheses)\n",
    "\n",
    "        encoded_premises = self._encoding_h(embedded_premises,\n",
    "                                          premises_lengths)\n",
    "        encoded_hypotheses = self._encoding_h(embedded_hypotheses,\n",
    "                                            hypotheses_lengths)\n",
    "        attended_premises, attended_hypotheses =            self._attention(encoded_premises,encoded_hypotheses,\n",
    "                            encoded_hypotheses, hypotheses_lengths),\\\n",
    "            self._attention(encoded_hypotheses,encoded_premises,\n",
    "                    encoded_premises,premises_lengths)\n",
    "        enhanced_premises = torch.cat([encoded_premises,\n",
    "                                       attended_premises,\n",
    "                                       encoded_premises - attended_premises,\n",
    "                                       encoded_premises * attended_premises],\n",
    "                                      dim=-1)\n",
    "        enhanced_hypotheses = torch.cat([encoded_hypotheses,\n",
    "                                         attended_hypotheses,\n",
    "                                         encoded_hypotheses -\n",
    "                                         attended_hypotheses,\n",
    "                                         encoded_hypotheses *\n",
    "                                         attended_hypotheses],\n",
    "                                        dim=-1)\n",
    "\n",
    "        projected_premises = self._projection(enhanced_premises)\n",
    "        projected_hypotheses = self._projection(enhanced_hypotheses)\n",
    "\n",
    "        if self.dropout:\n",
    "            projected_premises = self._rnn_dropout(projected_premises)\n",
    "            projected_hypotheses = self._rnn_dropout(projected_hypotheses)\n",
    "\n",
    "        v_ai = self._composition(torch.cat([projected_premises,premises],dim=-1), premises_lengths)\n",
    "        v_bj = self._composition(torch.cat([projected_hypotheses,hypotheses],dim=-1), hypotheses_lengths)\n",
    "#         v_ai=self.cond_p(v_ai,self.ln_product_cat(product_cat_embeddings))\n",
    "#         v_bj=self.cond_h(v_bj,self.ln_industry(industry_embeddings))\n",
    "        v_a_avg = torch.sum(v_ai * premises_mask.unsqueeze(1)\n",
    "                                                .transpose(2, 1), dim=1)\\\n",
    "            / torch.sum(premises_mask, dim=1, keepdim=True)\n",
    "        v_b_avg = torch.sum(v_bj * hypotheses_mask.unsqueeze(1)\n",
    "                                                  .transpose(2, 1), dim=1)\\\n",
    "            / torch.sum(hypotheses_mask, dim=1, keepdim=True)\n",
    "\n",
    "        v_a_max, _ = self.replace_masked(v_ai, premises_mask, -1e7).max(dim=1)\n",
    "        v_b_max, _ = self.replace_masked(v_bj, hypotheses_mask, -1e7).max(dim=1)\n",
    "\n",
    "        hidden = torch.cat([v_a_avg, v_a_max, v_b_avg, v_b_max], dim=1)\n",
    "\n",
    "        output_age=self.decoder_age(torch.cat([hidden,self.ln_tfidf(tfidf)],dim=-1))\n",
    "        output_gender=self.decoder_gender(torch.cat([hidden, self.ln_tfidf(tfidf)],dim=-1))\n",
    "        loss_age=LabelSmoothingCrossEntropy(0.1,weights=age_nums.to(self.device))\n",
    "        loss_gender=LabelSmoothingCrossEntropy(0.1,weights=gender_nums.to(self.device))\n",
    "#         loss_age=LabelSmoothingCrossEntropy(0,weights=age_nums.to(self.device))\n",
    "#         loss_gender=LabelSmoothingCrossEntropy(0,weights=gender_nums.to(self.device))\n",
    "        l=0.5*loss_age(output_age,age.long())+0.5*loss_gender(output_gender,gender.long())\n",
    "        return l,output_age,output_gender\n",
    "class ESIM_Single(nn.Module):\n",
    "\n",
    "    def __init__(self,config,\n",
    "                 dropout=0,\n",
    "                 device=\"cpu\"):\n",
    "        super(ESIM_Single, self).__init__()\n",
    "        embedding_dim=config.hidden_size\n",
    "        hidden_size=config.hidden_size\n",
    "        self.embedding_dim=embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        if self.dropout:\n",
    "            self._rnn_dropout = RNNDropout(p=self.dropout)\n",
    "\n",
    "\n",
    "#         self._encoding=TransEncoder(config)\n",
    "        self._encoding_h = Seq2SeqEncoder(nn.LSTM,\n",
    "                                        self.embedding_dim,\n",
    "                                        self.hidden_size,\n",
    "                                        bidirectional=True)\n",
    "#         self._encoding_p = Seq2SeqEncoder(nn.LSTM,\n",
    "#                                         self.embedding_dim,\n",
    "#                                         self.hidden_size,\n",
    "#                                         bidirectional=True)\n",
    "\n",
    "        self._attention =DotProductAttention()\n",
    "\n",
    "        self._projection = nn.Sequential(nn.Linear(2*4*self.hidden_size,\n",
    "                                                   self.hidden_size),\n",
    "                                         nn.ReLU())\n",
    "        self.time_embeddings = nn.Embedding(config.max_position_embeddings, 32)\n",
    "        self.click_times_embeddings = nn.Embedding(32, 32)\n",
    "        self.ad_age_embeddings = nn.Embedding(24, 32)\n",
    "        self.ad_gender_embeddings = nn.Embedding(24, 32)\n",
    "        self.advertiser_age_embeddings = nn.Embedding(24, 32)\n",
    "        self.advertiser_gender_embeddings = nn.Embedding(24, 32)\n",
    "\n",
    "        self._composition = Seq2SeqEncoder(nn.LSTM,\n",
    "                                           self.hidden_size*2,\n",
    "                                           self.hidden_size,\n",
    "                                           bidirectional=True)\n",
    "        self.ln_create=LayerNorm(50)\n",
    "        self.ln_ad=LayerNorm(50)\n",
    "        self.ln_product_id=LayerNorm(50)\n",
    "        self.ln_product_cat=LayerNorm(50)\n",
    "        self.ln_advertiser=LayerNorm(50)\n",
    "        self.ln_industry=LayerNorm(50)\n",
    "        \n",
    "        \n",
    "        self.ln_advertiser_id_industry=LayerNorm(50)\n",
    "        self.ln_product_cat_industry=LayerNorm(50)\n",
    "        self.ln_product_cat_advertiser=LayerNorm(50)\n",
    "        self.ln_product_id_advertiser=LayerNorm(50)\n",
    "        self.ln_product_id_industry=LayerNorm(50)\n",
    "        self.ln_product_id_product_cat=LayerNorm(50)\n",
    "\n",
    "        \n",
    "        self.ln_time=LayerNorm(32)\n",
    "        self.ln_click_times=LayerNorm(32)\n",
    "        self.ln_ad_age=LayerNorm(32)\n",
    "        self.ln_ad_gender=LayerNorm(32)\n",
    "        self.ln_advertiser_age=LayerNorm(32)\n",
    "        self.ln_advertiser_gender=LayerNorm(32)\n",
    "        \n",
    "        \n",
    "        self.ln_target_enc=LayerNorm(12)\n",
    "        self.ln_tfidf=LayerNorm(55)\n",
    "        self.ln_graph=LayerNorm(42)\n",
    "        self.ln_cal=LayerNorm(31)\n",
    "        self.ln_one_hot_target=LayerNorm(72)\n",
    "        self.ln_tfidf_sequence=LayerNorm(6)\n",
    "        self.ln_tr_sequence=LayerNorm(6)\n",
    "        self.decoder_gender=nn.Sequential(nn.Dropout(p=self.dropout),\n",
    "                                             nn.Linear(2*2*self.hidden_size+55,\n",
    "                                                       self.hidden_size),\n",
    "                                             nn.Tanh(),\n",
    "                                             nn.Dropout(p=self.dropout),\n",
    "                                             nn.Linear(self.hidden_size,\n",
    "                                                       2))\n",
    "        self.decoder_age=nn.Sequential(nn.Dropout(p=self.dropout),\n",
    "                                             nn.Linear(2*2*self.hidden_size+55,\n",
    "                                                       self.hidden_size),\n",
    "                                             nn.Tanh(),\n",
    "                                             nn.Dropout(p=self.dropout),\n",
    "                                             nn.Linear(self.hidden_size,\n",
    "                                                       10))\n",
    "\n",
    "        # Initialize all weights and biases in the model.\n",
    "        self.apply(_init_esim_weights)\n",
    "    def make_mask(self,X,valid_len):\n",
    "            shape=X.shape\n",
    "            if valid_len.dim()==1:\n",
    "                valid_len=valid_len.view(-1,1).repeat(1,shape[1])\n",
    "            mask=(torch.arange(0,X.shape[1]).repeat(X.shape[0],1).to(X.device)<valid_len).float()\n",
    "            return mask\n",
    "    def replace_masked(self,tensor, mask, value):\n",
    "        mask = mask.unsqueeze(1).transpose(2, 1)\n",
    "        reverse_mask = 1.0 - mask\n",
    "        values_to_add = value * reverse_mask\n",
    "        return tensor * mask + values_to_add\n",
    "    def forward(self,times,click_times,create_embeddings,ad_embeddings,product_id_embeddings,                product_cat_embeddings,advertiser_embeddings,industry_embeddings,target_encodings,                product_cat_industry_embeddings,product_cat_advertisers,product_id_advertiser_embeddings,                product_id_industry_embeddings,product_id_product_cats,                advertiser_id_industrys,target_embed_info,                length,cal,tfidf,tfidf_sequence,one_hot_target,tr_sequence,graph_sequence,                age,gender,encoder_hidden_states=None,encoder_extended_attention_mask=None):\n",
    "#                                                          self.ln_product_id_product_cat(product_id_product_cats),\n",
    "#                              self.ln_product_cat_advertiser(product_cat_advertisers),\\\n",
    "#                              self.ln_advertiser_id_industry(advertiser_id_industrys),\\\n",
    "#                              self.ln_graph(graph_sequence)\n",
    "#                               self.ln_advertiser_age(self.ad_age_embeddings(target_embed_info[8])),\\\n",
    "#                               self.ln_advertiser_gender(self.ad_gender_embeddings(target_embed_info[9]))\n",
    "#                            self.ln_product_id(product_id_embeddings[:,:,:]),\\\n",
    "        norm_time=self.ln_time(self.time_embeddings(times))\n",
    "        norm_click_time=self.click_times_embeddings(click_times)\n",
    "        norm_target=self.ln_one_hot_target(one_hot_target)\n",
    "        premises=torch.cat([self.ln_create(create_embeddings[:,:,:]),                            self.ln_ad(ad_embeddings[:,:,:]),                            self.ln_advertiser(advertiser_embeddings[:,:,:]),                            self.ln_product_cat_industry(product_cat_industry_embeddings[:,:,:]),                            self.ln_product_id_advertiser(product_id_advertiser_embeddings[:,:,:]),\n",
    "                            norm_target,norm_time,\\\n",
    "                            norm_click_time],dim=-1)\n",
    "        premises_lengths=length\n",
    "        premises_mask = self.make_mask(premises, premises_lengths).to(self.device)\n",
    "\n",
    "        embedded_premises = premises\n",
    "\n",
    "        if self.dropout:\n",
    "            embedded_premises = self._rnn_dropout(embedded_premises)\n",
    "\n",
    "        encoded_premises = self._encoding_h(embedded_premises,\n",
    "                                          premises_lengths)\n",
    "        attended_premises =self._attention(encoded_premises,encoded_premises,\n",
    "                            encoded_premises, premises_lengths)\n",
    "        enhanced_premises = torch.cat([encoded_premises,\n",
    "                                       attended_premises,\n",
    "                                       encoded_premises - attended_premises,\n",
    "                                       encoded_premises * attended_premises],\n",
    "                                      dim=-1)\n",
    "\n",
    "        projected_premises = self._projection(enhanced_premises)\n",
    "\n",
    "        if self.dropout:\n",
    "            projected_premises = self._rnn_dropout(projected_premises)\n",
    "\n",
    "        v_ai = self._composition(torch.cat([projected_premises,premises],dim=-1), premises_lengths)\n",
    "        v_a_avg = torch.sum(v_ai * premises_mask.unsqueeze(1)\n",
    "                                                .transpose(2, 1), dim=1)\\\n",
    "            / torch.sum(premises_mask, dim=1, keepdim=True)\n",
    "\n",
    "        v_a_max, _ = self.replace_masked(v_ai, premises_mask, -1e7).max(dim=1)\n",
    "\n",
    "        hidden = torch.cat([v_a_avg, v_a_max], dim=1)\n",
    "\n",
    "        output_age=self.decoder_age(torch.cat([hidden,self.ln_tfidf(tfidf)],dim=-1))\n",
    "        output_gender=self.decoder_gender(torch.cat([hidden, self.ln_tfidf(tfidf)],dim=-1))\n",
    "        loss_age=LabelSmoothingCrossEntropy(0.1,weights=age_nums.to(self.device))\n",
    "        loss_gender=LabelSmoothingCrossEntropy(0.1,weights=gender_nums.to(self.device))\n",
    "#         loss_age=LabelSmoothingCrossEntropy(0,weights=age_nums.to(self.device))\n",
    "#         loss_gender=LabelSmoothingCrossEntropy(0,weights=gender_nums.to(self.device))\n",
    "        l=0.5*loss_age(output_age,age.long())+0.5*loss_gender(output_gender,gender.long())\n",
    "        return l,output_age,output_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": [
     0,
     179
    ]
   },
   "outputs": [],
   "source": [
    "def train(args, train_dataset,val_dataset,temp_dataset, model,num_workers=0):\n",
    "    train_dataloader=Data.DataLoader(train_dataset,batch_size=args.train_batch_size,shuffle=True,\\\n",
    "                                     collate_fn=collate_fn,num_workers=num_workers)\n",
    "    model.train()\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay =[\"bias\", \"LayerNorm.weight\", \"gamma\",\"beta\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "#     opt = Lookahead(optimizer, k=5, alpha=0.5)\n",
    "#     opt = SWA(optimizer, swa_start=10, swa_freq=5)\n",
    "#     def lr_lambda(current_step):\n",
    "#         return max(\n",
    "#             0.0, float(115240 - (current_step+11524*2)) / float(max(1, 115240 - 11524))\n",
    "#         )\n",
    "\n",
    "#     scheduler=torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, -1)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")):\n",
    "        logger.info(\"  loading optimizer and scheduler...\")\n",
    "        # Load in optimizer and scheduler states\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "#         scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "    else:\n",
    "        logger.info(\"  No optimizer and scheduler,we build a new one\")        \n",
    "#     scheduler.step()\n",
    "#     print(optimizer.param_groups[0]['lr'])\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model,device_ids=args.card_list)\n",
    "\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "#     logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\n",
    "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "        args.train_batch_size\n",
    "        * args.gradient_accumulation_steps\n",
    "    )\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    epochs_trained = 0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    # Check if continuing training from a checkpoint\n",
    "    if os.path.exists(args.model_name_or_path):\n",
    "        # set global_step to global_step of last saved checkpoint from model path\n",
    "        try:\n",
    "            global_step = int(args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0])\n",
    "        except ValueError:\n",
    "            global_step = 0\n",
    "        epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "        steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "\n",
    "        logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "        logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "        logger.info(\"  Continuing training from global step %d\", global_step)\n",
    "        logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
    "\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = tqdm(range(\n",
    "        epochs_trained, int(args.num_train_epochs)), desc=\"Epoch\")\n",
    "    for _ in train_iterator:\n",
    "        start=time.time()\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                if  (step + 1) % args.gradient_accumulation_steps == 0: \n",
    "                        steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            batch = tuple(t.to(args.device) if not isinstance(t,list) else [e.to(args.device) for e in t]for t in batch[:])\n",
    "            inputs = { \"length\":batch[0],\"times\":batch[1],\"click_times\":batch[2],\"create_embeddings\": batch[3],\\\n",
    "                      \"ad_embeddings\": batch[4],\"product_id_embeddings\": batch[5],\"product_cat_embeddings\":batch[6],\\\n",
    "                      \"advertiser_embeddings\":batch[7],\"industry_embeddings\":batch[8],\"target_encodings\":batch[9],\\\n",
    "                      \"product_cat_industry_embeddings\":batch[10],\"product_id_advertiser_embeddings\":batch[11],\\\n",
    "                      \"product_id_industry_embeddings\":batch[12],\"cal\":batch[13],\"tfidf\":batch[14],\\\n",
    "                      \"tfidf_sequence\":batch[15],\"one_hot_target\":batch[16],\"tr_sequence\":batch[17],\"graph_sequence\":batch[18],\\\n",
    "                       \"advertiser_id_industrys\":batch[19],\"product_cat_advertisers\":batch[20],\"product_id_product_cats\":batch[21],\\\n",
    "                      \"target_embed_info\":batch[22],\"age\":batch[-2],\"gender\":batch[-1]}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "#             logger.info(\"  step:%d loss %.3f\", step,loss.item())\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "                optimizer.step()\n",
    "#                 opt.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "#                 print(optimizer.param_groups[0]['lr'])\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "\n",
    "        if True:\n",
    "#             if _==int(args.num_train_epochs)-1:\n",
    "#                 opt.swap_swa_sgd()\n",
    "            # Save model checkpoint\n",
    "#             evaluate(args, train_dataset,model,num_workers, prefix=\"train\")\n",
    "            if _==int(args.num_train_epochs)-1:\n",
    "                results,age_preds,age_out_label_ids,gender_preds,gender_out_label_ids =evaluate(args, val_dataset,model,num_workers,cross=True)\n",
    "            else:\n",
    "                results,age_preds,age_out_label_ids,gender_preds,gender_out_label_ids =evaluate(args, temp_dataset,model,num_workers,cross=True)\n",
    "            output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            model_to_save = (\n",
    "                model.module if hasattr(model, \"module\") else model\n",
    "            )  # Take care of distributed/parallel training\n",
    "            torch.save(model_to_save.state_dict(),os.path.join(output_dir,\"model.pt\"))\n",
    "            if _==int(args.num_train_epochs)-1:\n",
    "                np.save(os.path.join(output_dir,\"age_preds.npy\"),age_preds)\n",
    "                np.save(os.path.join(output_dir,\"gender_preds.npy\"),gender_preds)\n",
    "            logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "            torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "#             torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "            if args.fp16:\n",
    "                torch.save(amp.state_dict(),os.path.join(output_dir, \"amp.pt\"))\n",
    "            logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        print(time.time()-start)\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    return global_step, tr_loss / global_step\n",
    "def evaluate(args, eval_dataset,model,num_workers=0, prefix=\"\",cross=False):\n",
    "    eval_output_dir = args.output_dir \n",
    "\n",
    "    results = {}\n",
    "\n",
    "    if not os.path.exists(eval_output_dir) :\n",
    "        os.makedirs(eval_output_dir)\n",
    "    eval_dataloader=Data.DataLoader(eval_dataset,batch_size=args.eval_batch_size,shuffle=False,collate_fn=collate_fn,num_workers=num_workers)\n",
    "\n",
    "    # multi-gpu eval\n",
    "#         if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n",
    "#             model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    age_preds = None\n",
    "    age_out_label_ids = None\n",
    "    gender_preds = None\n",
    "    gender_out_label_ids = None\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(args.device) if not isinstance(t,list) else [e.to(args.device) for e in t]for t in batch[:])\n",
    "\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = { \"length\":batch[0],\"times\":batch[1],\"click_times\":batch[2],\"create_embeddings\": batch[3],\\\n",
    "                      \"ad_embeddings\": batch[4],\"product_id_embeddings\": batch[5],\"product_cat_embeddings\":batch[6],\\\n",
    "                      \"advertiser_embeddings\":batch[7],\"industry_embeddings\":batch[8],\"target_encodings\":batch[9],\\\n",
    "                      \"product_cat_industry_embeddings\":batch[10],\"product_id_advertiser_embeddings\":batch[11],\\\n",
    "                      \"product_id_industry_embeddings\":batch[12],\"cal\":batch[13],\"tfidf\":batch[14],\\\n",
    "                      \"tfidf_sequence\":batch[15],\"one_hot_target\":batch[16],\"tr_sequence\":batch[17],\"graph_sequence\":batch[18],\\\n",
    "                       \"advertiser_id_industrys\":batch[19],\"product_cat_advertisers\":batch[20],\"product_id_product_cats\":batch[21],\\\n",
    "                      \"target_embed_info\":batch[22],\"age\":batch[-2],\"gender\":batch[-1]}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, age_logits,gender_logits = outputs[:3]\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "            nb_eval_steps+=1\n",
    "        if age_preds is None:\n",
    "            age_preds = torch.softmax(age_logits,dim=-1).detach().cpu().numpy()\n",
    "            age_out_label_ids = inputs[\"age\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            age_preds = np.append(age_preds, torch.softmax(age_logits,dim=-1).detach().cpu().numpy(), axis=0)\n",
    "            age_out_label_ids = np.append(age_out_label_ids, inputs[\"age\"].detach().cpu().numpy(), axis=0)\n",
    "        if gender_preds is None:\n",
    "            gender_preds = torch.softmax(gender_logits,dim=-1).detach().cpu().numpy()\n",
    "            gender_out_label_ids = inputs[\"gender\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            gender_preds = np.append(gender_preds, torch.softmax(gender_logits,dim=-1).detach().cpu().numpy(), axis=0)\n",
    "            gender_out_label_ids = np.append(gender_out_label_ids, inputs[\"gender\"].detach().cpu().numpy(), axis=0)\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    if args.output_mode == \"classification\":\n",
    "        if not cross:\n",
    "            gender_preds = (gender_preds.argmax(axis=-1)).astype(np.int8)\n",
    "            age_preds = (age_preds.argmax(axis=-1)).astype(np.int8)\n",
    "        gender_out_label_ids=gender_out_label_ids.astype(np.int8)\n",
    "        age_out_label_ids=age_out_label_ids.astype(np.int8)\n",
    "    if not cross:\n",
    "        result = {\"acc_age\":(age_preds==age_out_label_ids).sum()/age_preds.shape[0],\\\n",
    "              \"acc_gender\":(gender_preds==gender_out_label_ids).sum()/gender_preds.shape[0]}\n",
    "    else:\n",
    "        temp_gender_preds = (gender_preds.argmax(axis=-1)).astype(np.int8)\n",
    "        temp_age_preds = (age_preds.argmax(axis=-1)).astype(np.int8)\n",
    "        result = {\"acc_age\":(temp_age_preds==age_out_label_ids).sum()/temp_age_preds.shape[0],\\\n",
    "              \"acc_gender\":(temp_gender_preds==gender_out_label_ids).sum()/temp_gender_preds.shape[0]}\n",
    "    results.update(result)\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"a\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    return results,age_preds,age_out_label_ids,gender_preds,gender_out_label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds=5\n",
    "kf = KFold(n_splits=n_folds, shuffle=False,random_state=20)\n",
    "train_idxs=[]\n",
    "val_idxs=[]\n",
    "for i,(train_idx,val_idx) in enumerate(kf.split(train_features)):\n",
    "    train_idxs.append(train_idx)\n",
    "    val_idxs.append(val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": [
     11
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangweilin/anaconda3/envs/fjw/lib/python3.6/site-packages/ipykernel_launcher.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d0839b1456493985da5b76283f9b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:56:35 - INFO - __main__ -     No optimizer and scheduler,we build a new one\n",
      "07/23/2020 10:56:35 - INFO - __main__ -   ***** Running training *****\n",
      "07/23/2020 10:56:35 - INFO - __main__ -     Num examples = 80\n",
      "07/23/2020 10:56:35 - INFO - __main__ -     Num Epochs = 1\n",
      "07/23/2020 10:56:35 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "07/23/2020 10:56:35 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "07/23/2020 10:56:35 - INFO - __main__ -     Total optimization steps = 1\n",
      "07/23/2020 10:56:35 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step\n",
      "07/23/2020 10:56:35 - INFO - __main__ -     Continuing training from epoch 0\n",
      "07/23/2020 10:56:35 - INFO - __main__ -     Continuing training from global step 0\n",
      "07/23/2020 10:56:35 - INFO - __main__ -     Will skip the first 0 steps in the first epoch\n",
      "/home/huangweilin/anaconda3/envs/fjw/lib/python3.6/site-packages/ipykernel_launcher.py:88: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e567108fb254727baa1fce6b07f4222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangweilin/anaconda3/envs/fjw/lib/python3.6/site-packages/ipykernel_launcher.py:91: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcdd66e4aeaa4d9e9135d8fb4353c3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=1, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:56:35 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "07/23/2020 10:56:35 - INFO - __main__ -     Num examples = 20\n",
      "07/23/2020 10:56:35 - INFO - __main__ -     Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangweilin/anaconda3/envs/fjw/lib/python3.6/site-packages/ipykernel_launcher.py:203: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27932299be641129ab942fa64f54d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:56:36 - INFO - __main__ -   ***** Eval results  *****\n",
      "07/23/2020 10:56:36 - INFO - __main__ -     acc_age = 1.0\n",
      "07/23/2020 10:56:36 - INFO - __main__ -     acc_gender = 1.0\n",
      "07/23/2020 10:56:36 - INFO - __main__ -   Saving model checkpoint to ../../model/fjw/model_ESIM_Single_glove50_1/checkpoint-1\n",
      "07/23/2020 10:56:36 - INFO - __main__ -   Saving optimizer and scheduler states to ../../model/fjw/model_ESIM_Single_glove50_1/checkpoint-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.1490545272827148\n",
      "\n",
      "第 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:56:36 - INFO - __main__ -     No optimizer and scheduler,we build a new one\n",
      "07/23/2020 10:56:36 - INFO - __main__ -   ***** Running training *****\n",
      "07/23/2020 10:56:36 - INFO - __main__ -     Num examples = 80\n",
      "07/23/2020 10:56:36 - INFO - __main__ -     Num Epochs = 1\n",
      "07/23/2020 10:56:36 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "07/23/2020 10:56:36 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "07/23/2020 10:56:36 - INFO - __main__ -     Total optimization steps = 1\n",
      "07/23/2020 10:56:36 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step\n",
      "07/23/2020 10:56:36 - INFO - __main__ -     Continuing training from epoch 0\n",
      "07/23/2020 10:56:36 - INFO - __main__ -     Continuing training from global step 0\n",
      "07/23/2020 10:56:36 - INFO - __main__ -     Will skip the first 0 steps in the first epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439817f126d640f78575cb4fc8845d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c9cce78ebb4f8ead4b43d37a57791c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=1, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:56:37 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "07/23/2020 10:56:37 - INFO - __main__ -     Num examples = 20\n",
      "07/23/2020 10:56:37 - INFO - __main__ -     Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd934f1d89d422099ef3fe084beb3c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:56:37 - INFO - __main__ -   ***** Eval results  *****\n",
      "07/23/2020 10:56:37 - INFO - __main__ -     acc_age = 1.0\n",
      "07/23/2020 10:56:37 - INFO - __main__ -     acc_gender = 1.0\n",
      "07/23/2020 10:56:37 - INFO - __main__ -   Saving model checkpoint to ../../model/fjw/model_ESIM_Single_glove50_2/checkpoint-1\n",
      "07/23/2020 10:56:37 - INFO - __main__ -   Saving optimizer and scheduler states to ../../model/fjw/model_ESIM_Single_glove50_2/checkpoint-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.120729684829712\n",
      "\n",
      "第 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:56:38 - INFO - __main__ -     No optimizer and scheduler,we build a new one\n",
      "07/23/2020 10:56:38 - INFO - __main__ -   ***** Running training *****\n",
      "07/23/2020 10:56:38 - INFO - __main__ -     Num examples = 80\n",
      "07/23/2020 10:56:38 - INFO - __main__ -     Num Epochs = 1\n",
      "07/23/2020 10:56:38 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "07/23/2020 10:56:38 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "07/23/2020 10:56:38 - INFO - __main__ -     Total optimization steps = 1\n",
      "07/23/2020 10:56:38 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step\n",
      "07/23/2020 10:56:38 - INFO - __main__ -     Continuing training from epoch 0\n",
      "07/23/2020 10:56:38 - INFO - __main__ -     Continuing training from global step 0\n",
      "07/23/2020 10:56:38 - INFO - __main__ -     Will skip the first 0 steps in the first epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd21a89e6834c61a345f13899342f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11926ca2d0bd4c17b5132c24e11e7028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=1, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:56:38 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "07/23/2020 10:56:38 - INFO - __main__ -     Num examples = 20\n",
      "07/23/2020 10:56:38 - INFO - __main__ -     Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6de9e3284a49618a417c941211a27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:56:39 - INFO - __main__ -   ***** Eval results  *****\n",
      "07/23/2020 10:56:39 - INFO - __main__ -     acc_age = 1.0\n",
      "07/23/2020 10:56:39 - INFO - __main__ -     acc_gender = 1.0\n",
      "07/23/2020 10:56:39 - INFO - __main__ -   Saving model checkpoint to ../../model/fjw/model_ESIM_Single_glove50_3/checkpoint-1\n",
      "07/23/2020 10:56:39 - INFO - __main__ -   Saving optimizer and scheduler states to ../../model/fjw/model_ESIM_Single_glove50_3/checkpoint-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.1566963195800781\n",
      "\n",
      "第 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:56:39 - INFO - __main__ -     No optimizer and scheduler,we build a new one\n",
      "07/23/2020 10:56:39 - INFO - __main__ -   ***** Running training *****\n",
      "07/23/2020 10:56:39 - INFO - __main__ -     Num examples = 80\n",
      "07/23/2020 10:56:39 - INFO - __main__ -     Num Epochs = 1\n",
      "07/23/2020 10:56:39 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "07/23/2020 10:56:39 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "07/23/2020 10:56:39 - INFO - __main__ -     Total optimization steps = 1\n",
      "07/23/2020 10:56:39 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step\n",
      "07/23/2020 10:56:39 - INFO - __main__ -     Continuing training from epoch 0\n",
      "07/23/2020 10:56:39 - INFO - __main__ -     Continuing training from global step 0\n",
      "07/23/2020 10:56:39 - INFO - __main__ -     Will skip the first 0 steps in the first epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1505ce67ec3f4c119135a23efdf290be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019c390aea804f149dd4ef98907ec34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=1, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:56:40 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "07/23/2020 10:56:40 - INFO - __main__ -     Num examples = 20\n",
      "07/23/2020 10:56:40 - INFO - __main__ -     Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26cdffce1879458fbee0016c4737c4e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:56:40 - INFO - __main__ -   ***** Eval results  *****\n",
      "07/23/2020 10:56:40 - INFO - __main__ -     acc_age = 1.0\n",
      "07/23/2020 10:56:40 - INFO - __main__ -     acc_gender = 1.0\n",
      "07/23/2020 10:56:40 - INFO - __main__ -   Saving model checkpoint to ../../model/fjw/model_ESIM_Single_glove50_4/checkpoint-1\n",
      "07/23/2020 10:56:40 - INFO - __main__ -   Saving optimizer and scheduler states to ../../model/fjw/model_ESIM_Single_glove50_4/checkpoint-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.1209242343902588\n",
      "\n",
      "第 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:56:40 - INFO - __main__ -     No optimizer and scheduler,we build a new one\n",
      "07/23/2020 10:56:40 - INFO - __main__ -   ***** Running training *****\n",
      "07/23/2020 10:56:40 - INFO - __main__ -     Num examples = 80\n",
      "07/23/2020 10:56:40 - INFO - __main__ -     Num Epochs = 1\n",
      "07/23/2020 10:56:40 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "07/23/2020 10:56:40 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "07/23/2020 10:56:40 - INFO - __main__ -     Total optimization steps = 1\n",
      "07/23/2020 10:56:40 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step\n",
      "07/23/2020 10:56:40 - INFO - __main__ -     Continuing training from epoch 0\n",
      "07/23/2020 10:56:40 - INFO - __main__ -     Continuing training from global step 0\n",
      "07/23/2020 10:56:40 - INFO - __main__ -     Will skip the first 0 steps in the first epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07db9a3ccc7c49cd97fa566e6031bf56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3669478cda134a4fb8e33c263882081e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=1, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:56:41 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "07/23/2020 10:56:41 - INFO - __main__ -     Num examples = 20\n",
      "07/23/2020 10:56:41 - INFO - __main__ -     Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ea6f8e112d4c3fb218b57f83531da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:56:41 - INFO - __main__ -   ***** Eval results  *****\n",
      "07/23/2020 10:56:41 - INFO - __main__ -     acc_age = 1.0\n",
      "07/23/2020 10:56:41 - INFO - __main__ -     acc_gender = 1.0\n",
      "07/23/2020 10:56:42 - INFO - __main__ -   Saving model checkpoint to ../../model/fjw/model_ESIM_Single_glove50_5/checkpoint-1\n",
      "07/23/2020 10:56:42 - INFO - __main__ -   Saving optimizer and scheduler states to ../../model/fjw/model_ESIM_Single_glove50_5/checkpoint-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.1525921821594238\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pattern='train'\n",
    "# config=BertConfig(**json.load(open(\"./ESIM_config.json\",\"r\")),\\\n",
    "#               output_hidden_states=True, output_attentions=True)\n",
    "# config.hidden_size=config.hidden_size-250\n",
    "config=BertConfig(**json.load(open(\"./ESIM_single.json\",\"r\")),\\\n",
    "              output_hidden_states=True, output_attentions=True)\n",
    "config.hidden_size=config.hidden_size-550\n",
    "device=torch.device(\"cuda:2\")\n",
    "age_preds_list=[]\n",
    "gender_preds_list=[]\n",
    "\n",
    "for fold in tqdm(range(5)):  \n",
    "    print(\"第\",fold)\n",
    "    train_idx=train_idxs[fold][:]\n",
    "    val_idx=val_idxs[fold][:]\n",
    "    train_dataset=UsrDataset(train_features.iloc[train_idx])\n",
    "    eval_dataset=UsrDataset(train_features.iloc[val_idx])\n",
    "    temp_dataset=UsrDataset(train_features.iloc[val_idx[:50000]])\n",
    "    learning_rate =0.003\n",
    "    weight_decay = 0.1\n",
    "    epochs =10\n",
    "    batch_size = 256\n",
    "    adam_epsilon=1e-8\n",
    "#     model=ESIM(config,0,device=device)\n",
    "    model=ESIM_Single(config,0,device=device)\n",
    "    output_dir=\"../../model/fjw/model_ESIM_Single_glove50_\"+str(fold+1)+\"/\"\n",
    "    card_list=[0,2,3]\n",
    "\n",
    "    args=ARG(train_batch_size=batch_size,eval_batch_size=batch_size,weight_decay=weight_decay,learning_rate=learning_rate,\n",
    "             adam_epsilon=adam_epsilon,num_train_epochs=epochs,warmup_steps=int(len(train_dataset)//(batch_size))*1,gradient_accumulation_steps=1,save_steps=int(len(train_dataset)//(batch_size)),\n",
    "             max_grad_norm=10.0,model_name_or_path=output_dir,output_dir=output_dir,seed=42,device=device,n_gpu=-1,\n",
    "            max_steps=0,output_mode=\"classification\",fp16=False,fp16_opt_level='O1',card_list=card_list)\n",
    "    if pattern=='train':\n",
    "#         model.load_state_dict(torch.load(os.path.join(output_dir,\"checkpoint-23048\",\"model.pt\")))\n",
    "        model=model.to(args.device)\n",
    "        train(args,train_dataset,eval_dataset,temp_dataset,model,4)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(os.path.join(output_dir,\"checkpoint-93750\",\"model.pt\")))\n",
    "        model=model.to(args.device)\n",
    "        results,age_preds,age_out_label_ids,gender_preds,gender_out_label_ids=\\\n",
    "        evaluate(args,temp_dataset,model,4,cross=True)\n",
    "        age_preds_list.append(age_preds)\n",
    "        gender_preds_list.append(gender_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test&val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:56:42 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "07/23/2020 10:56:42 - INFO - __main__ -     Num examples = 100\n",
      "07/23/2020 10:56:42 - INFO - __main__ -     Batch size = 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangweilin/anaconda3/envs/fjw/lib/python3.6/site-packages/ipykernel_launcher.py:203: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67fb679819141feb02f874555b70643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:56:43 - INFO - __main__ -   ***** Eval results  *****\n",
      "07/23/2020 10:56:43 - INFO - __main__ -     acc_age = 0.28\n",
      "07/23/2020 10:56:43 - INFO - __main__ -     acc_gender = 0.97\n",
      "07/23/2020 10:56:43 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "07/23/2020 10:56:43 - INFO - __main__ -     Num examples = 100\n",
      "07/23/2020 10:56:43 - INFO - __main__ -     Batch size = 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60c20af09204a8fba2d82176a27a30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:56:44 - INFO - __main__ -   ***** Eval results  *****\n",
      "07/23/2020 10:56:44 - INFO - __main__ -     acc_age = 0.16\n",
      "07/23/2020 10:56:44 - INFO - __main__ -     acc_gender = 0.99\n",
      "07/23/2020 10:56:44 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "07/23/2020 10:56:44 - INFO - __main__ -     Num examples = 100\n",
      "07/23/2020 10:56:44 - INFO - __main__ -     Batch size = 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125a1028a9684ce4bdbef74028eb86b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:56:44 - INFO - __main__ -   ***** Eval results  *****\n",
      "07/23/2020 10:56:44 - INFO - __main__ -     acc_age = 0.19\n",
      "07/23/2020 10:56:44 - INFO - __main__ -     acc_gender = 1.0\n",
      "07/23/2020 10:56:44 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "07/23/2020 10:56:44 - INFO - __main__ -     Num examples = 100\n",
      "07/23/2020 10:56:44 - INFO - __main__ -     Batch size = 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b2ebf586f14a40958ea4042142caaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:56:45 - INFO - __main__ -   ***** Eval results  *****\n",
      "07/23/2020 10:56:45 - INFO - __main__ -     acc_age = 0.13\n",
      "07/23/2020 10:56:45 - INFO - __main__ -     acc_gender = 1.0\n",
      "07/23/2020 10:56:45 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "07/23/2020 10:56:45 - INFO - __main__ -     Num examples = 100\n",
      "07/23/2020 10:56:45 - INFO - __main__ -     Batch size = 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c06c4659ae4b1a9c36e593ee6bbba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:56:45 - INFO - __main__ -   ***** Eval results  *****\n",
      "07/23/2020 10:56:45 - INFO - __main__ -     acc_age = 0.34\n",
      "07/23/2020 10:56:45 - INFO - __main__ -     acc_gender = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_features=pk.load(open(\"../../var/fjw/test_mid/new_feature_simple.pk\",\"rb\"))\n",
    "age_preds_list=[]\n",
    "gender_preds_list=[]\n",
    "# config=BertConfig(**json.load(open(\"./ESIM_config.json\",\"r\")),\\\n",
    "#               output_hidden_states=True, output_attentions=True)\n",
    "# config.hidden_size=config.hidden_size-250\n",
    "config=BertConfig(**json.load(open(\"./ESIM_single.json\",\"r\")),\\\n",
    "              output_hidden_states=True, output_attentions=True)\n",
    "config.hidden_size=config.hidden_size-550\n",
    "device=torch.device(\"cuda:2\")\n",
    "# model=ESIM(config,0,device=device)\n",
    "model=ESIM_Single(config,0,device=device)\n",
    "for fold in range(5):\n",
    "    print(\"第\",fold)\n",
    "    test_dataset=UsrDataset(train_features)\n",
    "    learning_rate =0.003\n",
    "    weight_decay = 0.1\n",
    "    epochs =10\n",
    "    batch_size = 512\n",
    "    adam_epsilon=1e-8\n",
    "    output_dir=\"../../model/fjw/model_ESIM_Single_glove50_\"+str(fold+1)+\"/\"\n",
    "    card_list=[0,2,3]\n",
    "    args=ARG(train_batch_size=batch_size,eval_batch_size=batch_size,weight_decay=weight_decay,learning_rate=learning_rate,\n",
    "             adam_epsilon=adam_epsilon,num_train_epochs=epochs,warmup_steps=int(len(test_dataset)//(batch_size))*1,gradient_accumulation_steps=1,save_steps=int(len(test_dataset)//(batch_size)),\n",
    "             max_grad_norm=5.0,model_name_or_path=output_dir,output_dir=output_dir,seed=42,device=device,n_gpu=-1,\n",
    "            max_steps=0,output_mode=\"classification\",fp16=False,fp16_opt_level='O1',card_list=card_list)\n",
    "\n",
    "    model.load_state_dict(torch.load(os.path.join(output_dir,\"checkpoint-93750\",\"model.pt\")))\n",
    "    model=model.to(args.device)\n",
    "    results,age_preds,age_out_label_ids,gender_preds,gender_out_label_ids=\\\n",
    "    evaluate(args,test_dataset,model,4,cross=True)\n",
    "    age_preds_list.append(age_preds)\n",
    "    gender_preds_list.append(gender_preds)\n",
    "age_preds_list=np.stack(age_preds_list,axis=-1).mean(axis=-1)\n",
    "gender_preds_list=np.stack(gender_preds_list,axis=-1).mean(axis=-1)\n",
    "test_age_preds=age_preds_list\n",
    "test_gender_preds=gender_preds_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:57:32 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "07/23/2020 10:57:32 - INFO - __main__ -     Num examples = 20\n",
      "07/23/2020 10:57:32 - INFO - __main__ -     Batch size = 512\n",
      "/home/huangweilin/anaconda3/envs/fjw/lib/python3.6/site-packages/ipykernel_launcher.py:203: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c49dbd4f4c44e58b98b117d8c7d8bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:57:33 - INFO - __main__ -   ***** Eval results  *****\n",
      "07/23/2020 10:57:33 - INFO - __main__ -     acc_age = 0.05\n",
      "07/23/2020 10:57:33 - INFO - __main__ -     acc_gender = 0.6\n",
      "07/23/2020 10:57:33 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "07/23/2020 10:57:33 - INFO - __main__ -     Num examples = 20\n",
      "07/23/2020 10:57:33 - INFO - __main__ -     Batch size = 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2613619997ef4312a477e3d915adddac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:57:34 - INFO - __main__ -   ***** Eval results  *****\n",
      "07/23/2020 10:57:34 - INFO - __main__ -     acc_age = 0.1\n",
      "07/23/2020 10:57:34 - INFO - __main__ -     acc_gender = 0.7\n",
      "07/23/2020 10:57:34 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "07/23/2020 10:57:34 - INFO - __main__ -     Num examples = 20\n",
      "07/23/2020 10:57:34 - INFO - __main__ -     Batch size = 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625d092bab044dbdba51fa159613ca51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:57:34 - INFO - __main__ -   ***** Eval results  *****\n",
      "07/23/2020 10:57:34 - INFO - __main__ -     acc_age = 0.2\n",
      "07/23/2020 10:57:34 - INFO - __main__ -     acc_gender = 0.5\n",
      "07/23/2020 10:57:35 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "07/23/2020 10:57:35 - INFO - __main__ -     Num examples = 20\n",
      "07/23/2020 10:57:35 - INFO - __main__ -     Batch size = 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f468faa2b74a00b596f8b60eca9cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:57:35 - INFO - __main__ -   ***** Eval results  *****\n",
      "07/23/2020 10:57:35 - INFO - __main__ -     acc_age = 0.15\n",
      "07/23/2020 10:57:35 - INFO - __main__ -     acc_gender = 0.7\n",
      "07/23/2020 10:57:35 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "07/23/2020 10:57:35 - INFO - __main__ -     Num examples = 20\n",
      "07/23/2020 10:57:35 - INFO - __main__ -     Batch size = 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fce63471654d89ad72a370d9e2c1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2020 10:57:36 - INFO - __main__ -   ***** Eval results  *****\n",
      "07/23/2020 10:57:36 - INFO - __main__ -     acc_age = 0.15\n",
      "07/23/2020 10:57:36 - INFO - __main__ -     acc_gender = 0.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_features=pk.load(open(\"../../var/fjw/train_mid/new_feature_simple.pk\",\"rb\"))\n",
    "train_age_preds=np.zeros((train_features.shape[0],10))\n",
    "train_gender_preds=np.zeros((train_features.shape[0],2))\n",
    "# config=BertConfig(**json.load(open(\"./ESIM_config.json\",\"r\")),\\\n",
    "#               output_hidden_states=True, output_attentions=True)\n",
    "# config.hidden_size=config.hidden_size-250\n",
    "config=BertConfig(**json.load(open(\"./ESIM_single.json\",\"r\")),\\\n",
    "              output_hidden_states=True, output_attentions=True)\n",
    "config.hidden_size=config.hidden_size-550\n",
    "device=torch.device(\"cuda:3\")\n",
    "# model=ESIM(config,0,device=device)\n",
    "model=ESIM_Single(config,0,device=device)\n",
    "for fold in range(5):\n",
    "    print(\"第\",fold)\n",
    "    train_idx=train_idxs[fold][:]\n",
    "    val_idx=val_idxs[fold][:]\n",
    "    val_dataset=UsrDataset(train_features.iloc[val_idx])\n",
    "    learning_rate =0.003\n",
    "    weight_decay = 0.1\n",
    "    epochs =10\n",
    "    batch_size = 512\n",
    "    adam_epsilon=1e-8\n",
    "    output_dir=\"../../model/fjw/model_ESIM_Single_glove50_\"+str(fold+1)+\"/\"\n",
    "#     output_dir=\"../../model/fjw/model_ESIM_Single_wv200_\"+str(fold+1)+\"/\"\n",
    "    card_list=[0,2,3]\n",
    "    args=ARG(train_batch_size=batch_size,eval_batch_size=batch_size,weight_decay=weight_decay,learning_rate=learning_rate,\n",
    "             adam_epsilon=adam_epsilon,num_train_epochs=epochs,warmup_steps=int(len(test_dataset)//(batch_size))*1,gradient_accumulation_steps=1,save_steps=int(len(test_dataset)//(batch_size)),\n",
    "             max_grad_norm=5.0,model_name_or_path=output_dir,output_dir=output_dir,seed=42,device=device,n_gpu=-1,\n",
    "            max_steps=0,output_mode=\"classification\",fp16=False,fp16_opt_level='O1',card_list=card_list)\n",
    "\n",
    "    model.load_state_dict(torch.load(os.path.join(output_dir,\"checkpoint-93750\",\"model.pt\")))\n",
    "    model=model.to(args.device)\n",
    "    results,age_preds,age_out_label_ids,gender_preds,gender_out_label_ids=\\\n",
    "    evaluate(args,val_dataset,model,4,cross=True)\n",
    "    train_age_preds[val_idx]=age_preds[:train_age_preds[val_idx].shape[0]]\n",
    "    train_gender_preds[val_idx]=gender_preds[:train_gender_preds[val_idx].shape[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300, 10), (300, 2), (100, 10), (100, 2))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_age_preds.shape,train_gender_preds.shape,test_age_preds.shape,test_gender_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "outputs_dict={\"train_gender\":train_gender_preds,\"train_age\":train_age_preds,\"test_gender\":test_gender_preds,\"test_age\":test_age_preds}\n",
    "pk.dump(outputs_dict,open(\"../../var/model_ret_dicts/fjw/\"+output_dir.strip().split(\"/\")[-2][:-2]+\".pk\",\"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fjw",
   "language": "python",
   "name": "fjw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
