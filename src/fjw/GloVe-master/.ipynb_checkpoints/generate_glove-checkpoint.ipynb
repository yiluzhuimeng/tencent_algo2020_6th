{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import logging\n",
    "import os\n",
    "import gc\n",
    "LOG_FORMAT = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "logging.basicConfig(level=logging.INFO, format=LOG_FORMAT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 200维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_id_product_category\n",
      "product_id_advertiser_id\n",
      "product_id_industry\n",
      "product_category_advertiser_id\n",
      "product_category_industry\n",
      "advertiser_id_industry\n"
     ]
    }
   ],
   "source": [
    "mp_column_nembedding = {'creative_id' : 200, 'ad_id' : 200, 'product_id' : 100, 'product_category' : 100, 'advertiser_id' : 200, 'industry' : 100}\n",
    "cross_cols=['product_id','product_category','advertiser_id','industry']\n",
    "combine_cross_cols=[]\n",
    "for idx in range(len(cross_cols)):\n",
    "    for jdx in range(idx+1,len(cross_cols)):\n",
    "        print(cross_cols[idx]+\"_\"+cross_cols[jdx])\n",
    "        combine_cross_cols.append(cross_cols[idx]+\"_\"+cross_cols[jdx])\n",
    "        mp_column_nembedding[cross_cols[idx]+\"_\"+cross_cols[jdx]]=100\n",
    "cols=['creative_id','ad_id','advertiser_id','industry','product_category','product_id']\n",
    "cols=cols+combine_cross_cols+cross_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-22 22:00:31,069 - INFO - start feature creative_id\n",
      "2020-07-22 22:00:31,070 - INFO - load\n",
      "2020-07-22 22:00:31,102 - INFO - finish load\n",
      "2020-07-22 22:00:38,342 - INFO - start feature ad_id\n",
      "2020-07-22 22:00:38,344 - INFO - load\n",
      "2020-07-22 22:00:38,347 - INFO - finish load\n",
      "2020-07-22 22:00:44,145 - INFO - start feature advertiser_id\n",
      "2020-07-22 22:00:44,147 - INFO - load\n",
      "2020-07-22 22:00:44,157 - INFO - finish load\n",
      "2020-07-22 22:00:46,561 - INFO - start feature industry\n",
      "2020-07-22 22:00:46,563 - INFO - load\n",
      "2020-07-22 22:00:46,571 - INFO - finish load\n",
      "2020-07-22 22:00:46,815 - INFO - start feature product_category\n",
      "2020-07-22 22:00:46,817 - INFO - load\n",
      "2020-07-22 22:00:46,820 - INFO - finish load\n",
      "2020-07-22 22:00:46,969 - INFO - start feature product_id\n",
      "2020-07-22 22:00:46,971 - INFO - load\n",
      "2020-07-22 22:00:46,979 - INFO - finish load\n",
      "2020-07-22 22:00:47,574 - INFO - start feature product_id_product_category\n",
      "2020-07-22 22:00:47,577 - INFO - load\n",
      "2020-07-22 22:00:47,593 - INFO - finish load\n",
      "2020-07-22 22:00:48,163 - INFO - start feature product_id_advertiser_id\n",
      "2020-07-22 22:00:48,166 - INFO - load\n",
      "2020-07-22 22:00:48,179 - INFO - finish load\n",
      "2020-07-22 22:00:49,957 - INFO - start feature product_id_industry\n",
      "2020-07-22 22:00:49,959 - INFO - load\n",
      "2020-07-22 22:00:49,964 - INFO - finish load\n",
      "2020-07-22 22:00:50,811 - INFO - start feature product_category_advertiser_id\n",
      "2020-07-22 22:00:50,813 - INFO - load\n",
      "2020-07-22 22:00:50,818 - INFO - finish load\n",
      "2020-07-22 22:00:52,516 - INFO - start feature product_category_industry\n",
      "2020-07-22 22:00:52,518 - INFO - load\n",
      "2020-07-22 22:00:52,526 - INFO - finish load\n",
      "2020-07-22 22:00:52,865 - INFO - start feature advertiser_id_industry\n",
      "2020-07-22 22:00:52,867 - INFO - load\n",
      "2020-07-22 22:00:52,878 - INFO - finish load\n",
      "2020-07-22 22:00:54,906 - INFO - start feature product_id\n",
      "2020-07-22 22:00:54,908 - INFO - load\n",
      "2020-07-22 22:00:54,910 - INFO - finish load\n",
      "2020-07-22 22:00:55,465 - INFO - start feature product_category\n",
      "2020-07-22 22:00:55,468 - INFO - load\n",
      "2020-07-22 22:00:55,472 - INFO - finish load\n",
      "2020-07-22 22:00:55,642 - INFO - start feature advertiser_id\n",
      "2020-07-22 22:00:55,645 - INFO - load\n",
      "2020-07-22 22:00:55,648 - INFO - finish load\n",
      "2020-07-22 22:00:58,155 - INFO - start feature industry\n",
      "2020-07-22 22:00:58,157 - INFO - load\n",
      "2020-07-22 22:00:58,161 - INFO - finish load\n"
     ]
    }
   ],
   "source": [
    "se_list = []\n",
    "import gc\n",
    "for feature in cols:\n",
    "        logging.info('start feature %s' % feature)\n",
    "        logging.info('load')\n",
    "        sentence_info=pickle.load(open('../../../var/fjw/usr_seq/se_user_'+feature+'.pickle', 'rb'))\n",
    "        logging.info('finish load')\n",
    "        sentences = []\n",
    "        for a_user in sentence_info:\n",
    "            sentence = []\n",
    "            for item_id in a_user:\n",
    "                sentence.append(str(item_id))\n",
    "            sentences.append(\" \".join(sentence))\n",
    "        corpus_file_name = \"corpus_%s\" % feature\n",
    "        with open(corpus_file_name, \"a+\") as f:\n",
    "            for s in sentences:\n",
    "                f.write(s+\"\\n\")\n",
    "        del sentences,sentence_info\n",
    "        gc.collect() \n",
    "        os.system(\"sh ./demo.sh %s %s\"%(corpus_file_name,100 if feature not in mp_column_nembedding.keys() else mp_column_nembedding[feature]))\n",
    "\n",
    "        with open('./vectors_%s.txt' % corpus_file_name, 'r') as f:\n",
    "            data = f.read()\n",
    "        lines = data.split('\\n')\n",
    "        mp_id_vec = {}\n",
    "        for line in lines:\n",
    "            strs = line.split(\" \")\n",
    "            mp_id_vec[strs[0]] = list( map(lambda x : float(x), strs[1:]))\n",
    "        pickle.dump(pd.Series(mp_id_vec), open('../../../var/se_glove_%s_%ddim.pickle' % (feature,mp_column_nembedding[feature]), 'wb'))\n",
    "        del mp_id_vec,lines,data\n",
    "        gc.collect()\n",
    "        os.system(\"rm cooccurrence.* vectors* vocab.txt corpus*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_column_nembedding = {'creative_id' : 100, 'ad_id' : 100, 'product_id' : 100, 'product_category' : 100, 'advertiser_id' : 100, 'industry' : 100}\n",
    "\n",
    "# cols=['creative_id','ad_id','advertiser_id','industry','product_category','product_id']\n",
    "cols=['creative_id','ad_id','advertiser_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-22 22:01:00,504 - INFO - start feature creative_id\n",
      "2020-07-22 22:01:00,505 - INFO - load\n",
      "2020-07-22 22:01:00,508 - INFO - finish load\n",
      "2020-07-22 22:01:04,410 - INFO - start feature ad_id\n",
      "2020-07-22 22:01:04,412 - INFO - load\n",
      "2020-07-22 22:01:04,416 - INFO - finish load\n",
      "2020-07-22 22:01:08,100 - INFO - start feature advertiser_id\n",
      "2020-07-22 22:01:08,103 - INFO - load\n",
      "2020-07-22 22:01:08,106 - INFO - finish load\n"
     ]
    }
   ],
   "source": [
    "se_list = []\n",
    "\n",
    "for feature in cols:\n",
    "        logging.info('start feature %s' % feature)\n",
    "        logging.info('load')\n",
    "        sentence_info=pickle.load(open('../../../var/fjw/usr_seq/se_user_'+feature+'.pickle', 'rb'))\n",
    "        logging.info('finish load')\n",
    "        sentences = []\n",
    "        for a_user in sentence_info:\n",
    "            sentence = []\n",
    "            for item_id in a_user:\n",
    "                sentence.append(str(item_id))\n",
    "            sentences.append(\" \".join(sentence))\n",
    "        corpus_file_name = \"corpus_%s\" % feature\n",
    "        with open(corpus_file_name, \"a+\") as f:\n",
    "            for s in sentences:\n",
    "                f.write(s+\"\\n\")\n",
    "        del sentences,sentence_info\n",
    "        gc.collect() \n",
    "        os.system(\"sh ./demo.sh %s %s\"%(corpus_file_name,100 if feature not in mp_column_nembedding.keys() else mp_column_nembedding[feature]))\n",
    "\n",
    "        with open('vectors_%s.txt' % corpus_file_name, 'r') as f:\n",
    "            data = f.read()\n",
    "        lines = data.split('\\n')\n",
    "        mp_id_vec = {}\n",
    "        for line in lines:\n",
    "            strs = line.split(\" \")\n",
    "            mp_id_vec[strs[0]] = list( map(lambda x : float(x), strs[1:]))\n",
    "        pickle.dump(pd.Series(mp_id_vec), open('../../../var/se_glove_%s_100dim.pickle' % feature, 'wb'))\n",
    "        del mp_id_vec,lines,data\n",
    "        gc.collect()\n",
    "        os.system(\"rm cooccurrence.* vectors* vocab.txt corpus*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_id_product_category\n",
      "product_id_advertiser_id\n",
      "product_id_industry\n",
      "product_category_advertiser_id\n",
      "product_category_industry\n",
      "advertiser_id_industry\n"
     ]
    }
   ],
   "source": [
    "cross_cols=['product_id','product_category','advertiser_id','industry']\n",
    "combine_cross_cols=[]\n",
    "for idx in range(len(cross_cols)):\n",
    "    for jdx in range(idx+1,len(cross_cols)):\n",
    "        print(cross_cols[idx]+\"_\"+cross_cols[jdx])\n",
    "        combine_cross_cols.append(cross_cols[idx]+\"_\"+cross_cols[jdx])\n",
    "mp_column_nembedding = {'creative_id' : 50, 'ad_id' : 50, 'product_id' : 50, 'product_category' : 50, 'advertiser_id' : 50, 'industry' : 50}\n",
    "\n",
    "cols=['creative_id','ad_id','advertiser_id','industry','product_category','product_id']\n",
    "cols=cols+combine_cross_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-22 22:01:28,703 - INFO - start feature creative_id\n",
      "2020-07-22 22:01:28,704 - INFO - load\n",
      "2020-07-22 22:01:28,708 - INFO - finish load\n",
      "2020-07-22 22:01:31,386 - INFO - start feature ad_id\n",
      "2020-07-22 22:01:31,388 - INFO - load\n",
      "2020-07-22 22:01:31,392 - INFO - finish load\n",
      "2020-07-22 22:01:33,955 - INFO - start feature advertiser_id\n",
      "2020-07-22 22:01:33,958 - INFO - load\n",
      "2020-07-22 22:01:33,962 - INFO - finish load\n",
      "2020-07-22 22:01:35,103 - INFO - start feature industry\n",
      "2020-07-22 22:01:35,105 - INFO - load\n",
      "2020-07-22 22:01:35,107 - INFO - finish load\n",
      "2020-07-22 22:01:35,328 - INFO - start feature product_category\n",
      "2020-07-22 22:01:35,330 - INFO - load\n",
      "2020-07-22 22:01:35,332 - INFO - finish load\n",
      "2020-07-22 22:01:35,484 - INFO - start feature product_id\n",
      "2020-07-22 22:01:35,486 - INFO - load\n",
      "2020-07-22 22:01:35,489 - INFO - finish load\n",
      "2020-07-22 22:01:35,904 - INFO - start feature product_id_product_category\n",
      "2020-07-22 22:01:35,906 - INFO - load\n",
      "2020-07-22 22:01:35,913 - INFO - finish load\n",
      "2020-07-22 22:01:36,361 - INFO - start feature product_id_advertiser_id\n",
      "2020-07-22 22:01:36,363 - INFO - load\n",
      "2020-07-22 22:01:36,367 - INFO - finish load\n",
      "2020-07-22 22:01:37,616 - INFO - start feature product_id_industry\n",
      "2020-07-22 22:01:37,618 - INFO - load\n",
      "2020-07-22 22:01:37,624 - INFO - finish load\n",
      "2020-07-22 22:01:38,247 - INFO - start feature product_category_advertiser_id\n",
      "2020-07-22 22:01:38,250 - INFO - load\n",
      "2020-07-22 22:01:38,255 - INFO - finish load\n",
      "2020-07-22 22:01:39,502 - INFO - start feature product_category_industry\n",
      "2020-07-22 22:01:39,504 - INFO - load\n",
      "2020-07-22 22:01:39,509 - INFO - finish load\n",
      "2020-07-22 22:01:39,822 - INFO - start feature advertiser_id_industry\n",
      "2020-07-22 22:01:39,825 - INFO - load\n",
      "2020-07-22 22:01:39,830 - INFO - finish load\n"
     ]
    }
   ],
   "source": [
    "se_list = []\n",
    "\n",
    "for feature in cols:\n",
    "        logging.info('start feature %s' % feature)\n",
    "        logging.info('load')\n",
    "        sentence_info=pickle.load(open('../../../var/fjw/usr_seq/se_user_'+feature+'.pickle', 'rb'))\n",
    "        logging.info('finish load')\n",
    "        sentences = []\n",
    "        for a_user in sentence_info:\n",
    "            sentence = []\n",
    "            for item_id in a_user:\n",
    "                sentence.append(str(item_id))\n",
    "            sentences.append(\" \".join(sentence))\n",
    "        corpus_file_name = \"corpus_%s\" % feature\n",
    "        with open(corpus_file_name, \"a+\") as f:\n",
    "            for s in sentences:\n",
    "                f.write(s+\"\\n\")\n",
    "        del sentences,sentence_info\n",
    "        gc.collect() \n",
    "        os.system(\"sh ./demo.sh %s %s\"%(corpus_file_name,50 if feature not in mp_column_nembedding.keys() else mp_column_nembedding[feature]))\n",
    "\n",
    "        with open('vectors_%s.txt' % corpus_file_name, 'r') as f:\n",
    "            data = f.read()\n",
    "        lines = data.split('\\n')\n",
    "        mp_id_vec = {}\n",
    "        for line in lines:\n",
    "            strs = line.split(\" \")\n",
    "            mp_id_vec[strs[0]] = list( map(lambda x : float(x), strs[1:]))\n",
    "        pickle.dump(pd.Series(mp_id_vec), open('../../../var/se_glove_%s_50dim.pickle' % feature, 'wb'))\n",
    "        del mp_id_vec,lines,data\n",
    "        gc.collect()\n",
    "        os.system(\"rm cooccurrence.* vectors* vocab.txt corpus*\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fjw",
   "language": "python",
   "name": "fjw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
